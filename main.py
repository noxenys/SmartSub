import re
import os
import yaml
import threading
import base64
import requests
import concurrent.futures
import datetime
import time
import random

from loguru import logger
from tqdm import tqdm
from retry import retry
from urllib.parse import quote, urlencode, urlparse
from pre_check import pre_check, get_sub_all

class SubscriptionCollector:
    def __init__(self):
        # 1. åˆå§‹åŒ–è·¯å¾„ (ä½¿ç”¨ç»å¯¹è·¯å¾„)
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        # åˆ‡æ¢å·¥ä½œç›®å½•åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç¡®ä¿ pre_check ç­‰å¤–éƒ¨æ¨¡å—èƒ½æ­£ç¡®åˆ›å»ºç›®å½•
        os.chdir(self.base_dir)
        
        self.config_path = os.path.join(self.base_dir, 'config.yaml')
        self.blacklist_path = os.path.join(self.base_dir, 'blacklist.txt')
        self.collected_nodes_path = os.path.join(self.base_dir, 'collected_nodes.txt')
        self.failed_log_path = os.path.join(self.base_dir, 'failed_subscriptions.log')
        
        # 2. åˆå§‹åŒ–æ•°æ®å®¹å™¨
        self.new_sub_list = []
        self.new_clash_list = []
        self.new_v2_list = []
        self.play_list = []
        self.airport_list = []
        self.collected_nodes_set = set()
        self.failed_sub_list = []
        
        # 3. è´¨é‡æ§åˆ¶ä¸ç»Ÿè®¡
        self.quality_stats = {
            'total_checked': 0,
            'low_quality': 0,
            'empty_subscription': 0,
            'spam_content': 0
        }
        self.lock = threading.Lock()
        
        # 4. æ­£åˆ™è¡¨è¾¾å¼
        self.re_str = r"https?://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]"
        self.node_str = r'(?:vmess|ss|ssr|trojan|vless|hysteria|hysteria2)://[-a-zA-Z0-9+/=@#?&._%[\]:]+'
        self.check_node_url_str = "https://{}/sub?target={}&url={}&insert=false&config=config%2FACL4SSR.ini"
        
        # 5. é…ç½®å‚æ•° (é»˜è®¤å€¼)
        self.max_workers = 32
        self.content_limit_mb = 3
        self.request_timeout = 15
        self.min_nodes = 3
        self.enable_quality_check = True
        self.check_url_list = []
        
        # 6. User-Agent åˆ—è¡¨ (æŠ—å°é” - æ‰©å±•æ± )
        self.user_agents = [
            # Chrome
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            # Edge
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/124.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Edg/124.0.0.0 Safari/537.36",
            # Firefox
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:125.0) Gecko/20100101 Firefox/125.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:125.0) Gecko/20100101 Firefox/125.0",
            # Safari
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Safari/605.1.15",
            # Mobile
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Mobile Safari/537.36"
        ]

        # 7. é™æ€èµ„æºåç¼€ (ç”¨äºè¿‡æ»¤æ— æ•ˆé“¾æ¥)
        self.static_extensions = (
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.ico', '.svg', 
            '.css', '.js', '.woff', '.woff2', '.ttf', '.eot', '.otf',
            '.mp3', '.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv',
            '.zip', '.rar', '.7z', '.tar', '.gz', '.iso', '.dmg', '.exe', '.apk'
        )
        
        # 8. ä»£ç†é…ç½® (æ”¯æŒ GitHub Actions ç­‰ç¯å¢ƒ)
        self.proxies = self._get_system_proxies()
        
        self.list_tg = []
        self.list_subscribe = []
        self.list_web_fuzz = []
        
        # åŠ è½½é…ç½®
        self.load_config()

    def get_abs_path(self, relative_path):
        """å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºåŸºäºè„šæœ¬ç›®å½•çš„ç»å¯¹è·¯å¾„"""
        if os.path.isabs(relative_path):
            return relative_path
        return os.path.join(self.base_dir, relative_path)

    @logger.catch
    def load_config(self):
        if not os.path.exists(self.config_path):
            logger.error(f"Config file not found: {self.config_path}")
            return

        with open(self.config_path, encoding="UTF-8") as f:
            data = yaml.safe_load(f)
        
        # è¯»å–æ€§èƒ½é…ç½®
        performance = data.get('performance', {})
        self.max_workers = performance.get('max_workers', 32)
        self.content_limit_mb = performance.get('content_limit_mb', 3)
        self.request_timeout = performance.get('request_timeout', 15)
        
        # è¯»å–è´¨é‡æ§åˆ¶é…ç½®
        quality = data.get('quality_control', {})
        self.min_nodes = quality.get('min_nodes', 3)
        self.enable_quality_check = quality.get('enable_quality_check', True)
        
        # èŠ‚ç‚¹çº§å»é‡æ± 
        self.unique_nodes = set()
        
        logger.info(f'æ€§èƒ½é…ç½®: çº¿ç¨‹æ•°={self.max_workers}, é™åˆ¶={self.content_limit_mb}MB, è¶…æ—¶={self.request_timeout}s')
        logger.info(f'è´¨é‡æ§åˆ¶: æœ€å°‘èŠ‚ç‚¹={self.min_nodes}, è´¨æ£€={self.enable_quality_check}')
        
        # è·å– Telegram é¢‘é“
        list_tg_raw = data.get('tgchannel', [])
        self.list_tg = []
        for url in list_tg_raw:
            url = str(url).strip()
            if not url:
                continue
                
            # ä½¿ç”¨æ­£åˆ™æ™ºèƒ½æå–é¢‘é“ ID
            # åŒ¹é…: t.me/channel, t.me/s/channel, telegram.me/channel
            # èƒ½å¤Ÿå¤„ç†æœ«å°¾æ–œæ ã€å‚æ•°ç­‰æƒ…å†µ
            match = re.search(r'(?:t\.me|telegram\.me)/(?:s/)?([a-zA-Z0-9_]+)', url, re.IGNORECASE)
            
            if match:
                channel_id = match.group(1)
                # æ’é™¤ä¸€äº›éé¢‘é“çš„ç³»ç»Ÿè·¯å¾„
                if channel_id.lower() not in ['s', 'share', 'joinchat', 'addstickers', 'iv']:
                    self.list_tg.append(f'https://t.me/s/{channel_id}')
            elif '/' not in url and '@' not in url:
                # æ”¯æŒçº¯é¢‘é“å: channel_name
                self.list_tg.append(f'https://t.me/s/{url}')
            elif url.startswith('@'):
                # æ”¯æŒ @channel_name
                self.list_tg.append(f'https://t.me/s/{url[1:]}')
            else:
                logger.warning(f'å¿½ç•¥æ— æ³•è§£æçš„ Telegram é“¾æ¥: {url}')
        
        self.list_subscribe = data.get('subscribe', [])
        self.list_web_fuzz = data.get('web_pages', [])

        # è·å–è®¢é˜…è½¬æ¢ API
        # ä¼˜å…ˆè¯»å– subconverter_backendsï¼Œå…¼å®¹æ—§é…ç½® sub_convert_apis
        config_apis = data.get('subconverter_backends') or data.get('sub_convert_apis', [])
        if config_apis:
            self.check_url_list = config_apis
            logger.info(f'å·²åŠ è½½ {len(self.check_url_list)} ä¸ªè®¢é˜…è½¬æ¢ API')
        else:
            logger.warning('æœªé…ç½® subconverter_backendsï¼Œå°†ä½¿ç”¨é»˜è®¤ API')
            # æä¾›ä¸€ç»„å†…ç½®çš„é»˜è®¤ API é˜²æ­¢ç¨‹åºå‡ºé”™
            self.check_url_list = ['api.dler.io','sub.xeton.dev','sub.id9.cc','sub.maoxiongnet.com']

    @logger.catch
    def load_sub_yaml(self, path_yaml):
        abs_path = self.get_abs_path(path_yaml)
        if os.path.isfile(abs_path):
            with open(abs_path, encoding="UTF-8") as f:
                dict_url = yaml.safe_load(f)
        else:
            dict_url = {
                "æœºåœºè®¢é˜…": [],
                "clashè®¢é˜…": [],
                "v2è®¢é˜…": [],
                "å¼€å¿ƒç©è€": []
            }
        logger.info(f'è¯»å–æ–‡ä»¶æˆåŠŸ: {abs_path}')
        return dict_url

    def get_random_ua(self):
        """éšæœºè·å– User-Agent"""
        return random.choice(self.user_agents)

    def mask_url(self, url):
        """å¯¹ URL è¿›è¡Œè„±æ•å¤„ç†ï¼Œéšè—æ•æ„Ÿå‚æ•°"""
        if not url: return ""
        # å¸¸è§æ•æ„Ÿå‚æ•°
        sensitive_keys = ['token', 'key', 'uuid', 'access_token', 'secret', 'auth']
        try:
            masked_url = url
            for key in sensitive_keys:
                # åŒ¹é… ?key=value æˆ– &key=value
                pattern = f'([?&]{key}=)([^&]+)'
                masked_url = re.sub(pattern, r'\1******', masked_url, flags=re.IGNORECASE)
            return masked_url
        except Exception:
            return "******"

    def check_ssrf(self, url):
        """ç®€å•çš„ SSRF é˜²å¾¡æ£€æµ‹"""
        if not url: return False
        try:
            url_lower = url.lower()
            # ç®€å•åˆ¤æ–­æ˜¯å¦ä»¥ localhost æˆ– 127.0.0.1 å¼€å¤´
            if url_lower.startswith(('http://localhost', 'https://localhost', 
                                   'http://127.0.0.1', 'https://127.0.0.1')):
                logger.warning(f'æ‹¦æˆªæ½œåœ¨çš„ SSRF è¯·æ±‚: {self.mask_url(url)}')
                return False
            return True
        except Exception:
            return False

    def _get_system_proxies(self):
        """ä»ç¯å¢ƒå˜é‡è·å–ä»£ç†è®¾ç½®"""
        proxies = {}
        http_proxy = os.environ.get('HTTP_PROXY') or os.environ.get('http_proxy')
        https_proxy = os.environ.get('HTTPS_PROXY') or os.environ.get('https_proxy')
        
        if http_proxy:
            proxies['http'] = http_proxy
        if https_proxy:
            proxies['https'] = https_proxy
            
        if proxies:
            logger.info(f'å·²æ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†è®¾ç½®: {proxies}')
        return proxies if proxies else None

    @logger.catch
    def fetch_urls_from_page(self, url):
        """é€šç”¨ç½‘é¡µæŠ“å–å‡½æ•° (å¢å¼ºæŠ—å°é”)"""
        if not self.check_ssrf(url):
            return []

        # é’ˆå¯¹ Telegram é¢‘é“çš„ä¼˜åŒ–ï¼šä¸é‡è¯•ï¼Œå¿«é€Ÿè·³è¿‡
        is_tg_channel = 't.me/s/' in url
        # TG é¢‘é“: 1 æ¬¡å°è¯• (0 æ¬¡é‡è¯•)
        # æ™®é€šé“¾æ¥: 2 æ¬¡å°è¯• (1 æ¬¡é‡è¯•) - ä» 3 æ”¹ä¸º 2ï¼Œæé«˜æ•ˆç‡
        max_attempts = 1 if is_tg_channel else 2

        url_list = []
        node_list = []
        data = None
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(max_attempts):
            try:
                headers = {
                    'User-Agent': self.get_random_ua()
                }
                
                # å‘èµ·è¯·æ±‚ (å¯ç”¨ stream æ¨¡å¼é˜²æ­¢å†…å­˜æº¢å‡º)
                resp = requests.get(url, headers=headers, timeout=self.request_timeout, proxies=self.proxies, stream=True)
                
                # é’ˆå¯¹ 403/429 çš„ç‰¹æ®Šé‡è¯•é€»è¾‘
                if resp.status_code in [403, 429]:
                    resp.close()
                    if attempt < max_attempts - 1:
                        wait_time = random.uniform(1, 3)
                        msg = f'{self.mask_url(url)}\té‡åˆ° {resp.status_code}'
                        if resp.status_code == 403:
                            msg += ' (å¯èƒ½ IP è¢«å±è”½)'
                        logger.warning(f'{msg}ï¼Œç­‰å¾… {wait_time:.1f}s åæ›´æ¢ UA é‡è¯•...')
                        time.sleep(wait_time)
                        continue
                    else:
                        # TG é¢‘é“é‡åˆ°é™åˆ¶ç›´æ¥è·³è¿‡ï¼Œä¸è¾“å‡ºè­¦å‘Šï¼Œå‡å°‘æ—¥å¿—å™ªéŸ³
                        if not is_tg_channel:
                            logger.warning(f'{self.mask_url(url)}\té‡åˆ° {resp.status_code}ï¼Œé‡è¯•æ¬¡æ•°è€—å°½')
                        return []
                
                # é’ˆå¯¹ 404 çš„å¿«é€Ÿå¤±è´¥é€»è¾‘
                if resp.status_code == 404:
                    resp.close()
                    logger.warning(f'{self.mask_url(url)}\tèµ„æºä¸å­˜åœ¨ (404)ï¼Œè·³è¿‡')
                    return []
                    
                # é’ˆå¯¹ 500/502/503/504 çš„æœåŠ¡å™¨é”™è¯¯é‡è¯•é€»è¾‘
                if resp.status_code >= 500:
                    resp.close()
                    if attempt < max_attempts - 1:
                        wait_time = random.uniform(2, 5) # æœåŠ¡å™¨é”™è¯¯å¤šç­‰ä¸€ä¼šå„¿
                        logger.warning(f'{self.mask_url(url)}\tæœåŠ¡å™¨é”™è¯¯ {resp.status_code}ï¼Œç­‰å¾… {wait_time:.1f}s åé‡è¯•...')
                        time.sleep(wait_time)
                        continue
                    else:
                        logger.warning(f'{self.mask_url(url)}\tæœåŠ¡å™¨é”™è¯¯ {resp.status_code}ï¼Œé‡è¯•æ¬¡æ•°è€—å°½')
                        return []
                
                # æ­£å¸¸å“åº”å¤„ç†
                content_limit = self.content_limit_mb * 1024 * 1024
                content = b""
                download_failed = False
                
                try:
                    for chunk in resp.iter_content(chunk_size=8192):
                        content += chunk
                        if len(content) > content_limit:
                            logger.warning(f'{self.mask_url(url)}\tè¶…è¿‡å¤§å°é™åˆ¶({self.content_limit_mb}MB)ï¼Œæˆªæ–­ä¸‹è½½')
                            # download_failed = True  <-- ä¸éœ€è¦æ ‡è®°ä¸ºå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨æˆªæ–­åçš„å†…å®¹å°è¯•æå–
                            break
                except Exception as e:
                     logger.warning(f'{self.mask_url(url)}\tä¸‹è½½ä¸­æ–­: {e}')
                     download_failed = True

                resp.close()
                
                if download_failed and attempt < max_attempts - 1:
                    continue

                # å°è¯•è§£ç 
                data = content.decode('utf-8', errors='ignore')
                break # æˆåŠŸè·å–ï¼Œè·³å‡ºå¾ªç¯
                
            except requests.RequestException as e:
                if attempt < max_attempts - 1:
                    time.sleep(1)
                    continue
                else:
                    if not is_tg_channel:
                         logger.warning(f'{self.mask_url(url)}\tç½‘ç»œè¯·æ±‚å¤±è´¥: {type(e).__name__}')
                    return []
            except Exception as e:
                logger.error(f'{self.mask_url(url)}\tå¤„ç†å¤±è´¥: {type(e).__name__} - {str(e)}')
                return []
        
        if not data:
            return []

        try:
            # 1. æå–è®¢é˜… URL
            all_url_list = re.findall(self.re_str, data)
            
            filter_string_list = ["//t.me/", "cdn-telegram.org", "w3.org", "google.com", "github.com/site", "github.com/features", "cdn5.telesco.pe"]
            url_list = [item for item in all_url_list if not any(filter_string in item for filter_string in filter_string_list)]
            
            # è¿‡æ»¤é™æ€èµ„æº
            url_list = [item for item in url_list if not item.lower().endswith(self.static_extensions)]
            
            url_list = list(set(url_list))

            # è¿‡æ»¤æ•æ„Ÿé“¾æ¥
            url_list = [u for u in url_list if self.is_safe_url(u)]

            # 2. æå–ç›´æ¥èŠ‚ç‚¹
            direct_nodes = re.findall(self.node_str, data)
            if direct_nodes:
                node_list.extend(direct_nodes)
                logger.info(f'{self.mask_url(url)}\tå‘ç° {len(direct_nodes)} ä¸ªç›´æ¥èŠ‚ç‚¹')

            if node_list:
                self.collected_nodes_set.update(node_list)

            # 3. è´¨é‡æ§åˆ¶
            if len(url_list) == 0 and len(node_list) == 0:
                logger.warning(f'{self.mask_url(url)}\tæ— æœ‰æ•ˆå†…å®¹')
                return []
            
            if len(url_list) + len(node_list) < 2:
                logger.warning(f'{self.mask_url(url)}\tå†…å®¹è¿‡å°‘({len(url_list) + len(node_list)} < 2)ï¼Œå·²è·³è¿‡')
                return []
            
            logger.info(f'{self.mask_url(url)}\tè·å–æˆåŠŸ\tè®¢é˜…é“¾æ¥:{len(url_list)} èŠ‚ç‚¹é“¾æ¥:{len(node_list)}')
        except Exception as e:
            logger.error(f'{self.mask_url(url)}\tæ•°æ®è§£æå¤±è´¥: {type(e).__name__} - {str(e)}')
        
        return url_list

    def is_safe_url(self, url):
        if not url: return False
        url_lower = url.lower()
        sensitive_patterns = [
            'glpat-', 'ghp_', 'gho_', 'ghu_', 'ghs_', 'ghr_', 
            'private-token', 'access_token=', 'secret='
        ]
        for pattern in sensitive_patterns:
            if pattern in url_lower:
                logger.warning(f'å‘ç°æ•æ„Ÿé“¾æ¥å¹¶å·²è¿‡æ»¤: {self.mask_url(url)[:30]}... (åŒ…å« {pattern})')
                return False
        return True

    def filter_base64(self, text):
        ss = ['ss://', 'vmess://', 'trojan://', 'vless://', 'hysteria2://']
        for i in ss:
            if i in text:
                return True
        return False

    def extract_nodes(self, content):
        """ä»å†…å®¹ä¸­æå–èŠ‚ç‚¹é“¾æ¥"""
        nodes = []
        try:
            # å°è¯•è§£æ Base64
            decoded_text = ""
            try:
                # ç®€å•çš„ Base64 æ¢æµ‹
                sample_length = min(256, len(content))
                head_text = content[:sample_length].strip()
                if not '://' in head_text and not 'proxies:' in head_text: # åªæœ‰ä¸åŒ…å«åè®®å¤´æ‰å°è¯•è§£ç 
                    missing_padding = len(content) % 4
                    if missing_padding:
                        content += '=' * (4 - missing_padding)
                    decoded_text = base64.b64decode(content).decode('utf-8', errors='ignore')
            except Exception:
                pass

            # ä»åŸå§‹å†…å®¹æå–
            nodes.extend(re.findall(self.node_str, content))
            
            # ä»è§£ç å†…å®¹æå–
            if decoded_text:
                nodes.extend(re.findall(self.node_str, decoded_text))
                
        except Exception:
            pass
        
        return list(set(nodes)) # å±€éƒ¨å»é‡

    def count_nodes_in_content(self, content, is_clash=False):
        try:
            if is_clash:
                data = yaml.safe_load(content)
                proxies = data.get('proxies', [])
                return len(proxies)
            else:
                try:
                    decoded = base64.b64decode(content).decode('utf-8', errors='ignore')
                    nodes = [line for line in decoded.split('\n') if line.strip() and '://' in line]
                    return len(nodes)
                except Exception:
                    return 0
        except Exception:
            return 0

    def validate_subscription_quality(self, url, content, is_clash=False):
        if not self.enable_quality_check:
            return True
        
        node_count = self.count_nodes_in_content(content, is_clash)
        
        if node_count == 0:
            logger.warning(f'{self.mask_url(url)}\tç©ºè®¢é˜…ï¼ˆ0ä¸ªèŠ‚ç‚¹ï¼‰- å·²è·³è¿‡')
            with self.lock:
                self.quality_stats['empty_subscription'] += 1
            return False
        
        if node_count < self.min_nodes:
            logger.warning(f'{self.mask_url(url)}\tèŠ‚ç‚¹è¿‡å°‘ï¼ˆ{node_count} < {self.min_nodes}ï¼‰- å·²è·³è¿‡')
            with self.lock:
                self.quality_stats['low_quality'] += 1
            return False
        
        spam_keywords = ['å·²è¿‡æœŸ', 'è¯·è´­ä¹°', 'è¯•ç”¨ç»“æŸ', 'è”ç³»å®¢æœ', 'å·²åˆ°æœŸ']
        content_lower = content.lower()
        if any(keyword in content_lower for keyword in spam_keywords):
            logger.warning(f'{self.mask_url(url)}\tæ£€æµ‹åˆ°åƒåœ¾å†…å®¹ - å·²è·³è¿‡')
            with self.lock:
                self.quality_stats['spam_content'] += 1
            return False
        
        logger.info(f'{self.mask_url(url)}\tè´¨é‡éªŒè¯é€šè¿‡ï¼ˆ{node_count} ä¸ªèŠ‚ç‚¹ï¼‰')
        return True

    @logger.catch
    def sub_check(self, url, bar):
        if not self.check_ssrf(url):
            bar.update(1)
            return

        # å¿«é€Ÿè¿‡æ»¤é™æ€èµ„æºåç¼€ (äºŒæ¬¡ä¿é™©)
        if url.lower().endswith(self.static_extensions):
            bar.update(1)
            return

        # 3æ¬¡é‡è¯•æœºåˆ¶ (æ›¿ä»£ @retry)
        for attempt in range(2):
            try:
                headers = {'User-Agent': self.get_random_ua()}
                
                res = requests.get(url, headers=headers, timeout=self.request_timeout, stream=True, proxies=self.proxies)
                
                # é’ˆå¯¹ 403/429 çš„ç‰¹æ®Šé‡è¯•é€»è¾‘
                if res.status_code in [403, 429]:
                    res.close()
                    if attempt < 1:
                        wait_time = random.uniform(1, 3)
                        msg = f'{self.mask_url(url)}\tæ£€æµ‹é‡åˆ° {res.status_code}'
                        if res.status_code == 403:
                            msg += ' (å¯èƒ½ IP è¢«å±è”½)'
                        logger.warning(f'{msg}ï¼Œç­‰å¾… {wait_time:.1f}s åæ›´æ¢ UA é‡è¯•...')
                        time.sleep(wait_time)
                        continue
                    else:
                        self.failed_sub_list.append(url)
                        logger.warning(f'{self.mask_url(url)}\tçŠ¶æ€ç å¼‚å¸¸: {res.status_code} (é‡è¯•è€—å°½)')
                        break
                
                if res.status_code == 200:
                    header_info_valid = False
                    header_play_info = ""

                    # Header Check
                    # æ³¨æ„ï¼šè·å–åˆ°æµé‡ä¿¡æ¯åä¸åº”ç›´æ¥è¿”å›ï¼Œå¿…é¡»ç»§ç»­æ‰§è¡Œ Body ä¸‹è½½å’ŒèŠ‚ç‚¹æå–ï¼Œ
                    # è¿™æ ·æ‰èƒ½ç¡®ä¿è¯¥è®¢é˜…ä¸­çš„èŠ‚ç‚¹è¢«è§£æå¹¶åŠ å…¥å»é‡æ± ã€‚
                    try: 
                        info = res.headers.get('subscription-userinfo')
                        if info:
                            info_num = re.findall(r'\d+', info)
                            if info_num:
                                upload = int(info_num[0])
                                download = int(info_num[1])
                                total = int(info_num[2])
                                unused = (total - upload - download) / 1024 / 1024 / 1024
                                unused_rounded = round(unused, 2)
                                if unused_rounded > 0:
                                    header_info_valid = True
                                    header_play_info = 'å¯ç”¨æµé‡:' + str(unused_rounded) + ' GB                    ' + url
                    except Exception:
                        pass

                    # Body Check
                    content_limit = self.content_limit_mb * 1024 * 1024
                    content = b""
                    try:
                        for chunk in res.iter_content(chunk_size=8192):
                            content += chunk
                            if len(content) > content_limit:
                                logger.debug(f'{self.mask_url(url)} è¶…è¿‡å¤§å°é™åˆ¶ï¼Œæˆªæ–­ä¸‹è½½')
                                break
                        text = content.decode('utf-8', errors='ignore')
                    except Exception:
                        res.close()
                        if attempt < 2: continue # ä¸‹è½½ä¸­æ–­å°è¯•é‡è¯•
                        break
                    finally:
                        res.close()

                    # è´¨é‡æ§åˆ¶ï¼šå†…å®¹å»é‡æ£€æŸ¥ (å·²åºŸå¼ƒæ–‡ä»¶çº§ MD5 å»é‡)
                    with self.lock:
                        self.quality_stats['total_checked'] += 1
                    
                    # è§£æèŠ‚ç‚¹å¹¶åŠ å…¥å…¨å±€å»é‡æ± 
                    nodes = self.extract_nodes(text)
                    if nodes:
                        with self.lock:
                            self.unique_nodes.update(nodes)

                    # Clash åˆ¤æ–­
                    is_clash = False
                    try:
                        if 'proxies:' in text:
                            is_clash = True
                            if not self.validate_subscription_quality(url, text, is_clash=True):
                                break # è´¨é‡ä¸è¾¾æ ‡ï¼Œä¸é‡è¯•
                            
                            with self.lock:
                                self.new_clash_list.append(url)
                                if header_info_valid:
                                    self.new_sub_list.append(url)
                                    self.play_list.append(header_play_info)
                            break # æˆåŠŸ
                    except Exception:
                        pass

                    # V2Ray/Base64 åˆ¤æ–­
                    try:
                        sample_length = min(256, len(text))
                        head_text = text[:sample_length].strip()
                        missing_padding = len(head_text) % 4
                        if missing_padding:
                            head_text += '=' * (4 - missing_padding)
                        
                        decoded_text = base64.b64decode(head_text).decode('utf-8', errors='ignore')
                        if self.filter_base64(decoded_text):
                            if not self.validate_subscription_quality(url, text, is_clash=False):
                                break # è´¨é‡ä¸è¾¾æ ‡ï¼Œä¸é‡è¯•
                            
                            with self.lock:
                                self.new_v2_list.append(url)
                                if header_info_valid:
                                    self.new_sub_list.append(url)
                                    self.play_list.append(header_play_info)
                    except Exception:
                        pass
                    
                    # æˆåŠŸå¤„ç†å®Œæ¯•ï¼ˆå³ä½¿æ²¡åŒ¹é…åˆ°ä»»ä½•ç±»å‹ï¼Œä¹Ÿè§†ä¸º200å“åº”å¤„ç†ç»“æŸï¼‰
                    break
                else:
                    # é 200, 403, 429 çš„å…¶ä»–é”™è¯¯ (å¦‚ 404, 500)
                    res.close()
                    if attempt < 2 and res.status_code >= 500:
                        # 5xx é”™è¯¯å¯ä»¥é‡è¯•
                        continue
                    
                    self.failed_sub_list.append(url)
                    logger.warning(f'{self.mask_url(url)}\tçŠ¶æ€ç å¼‚å¸¸: {res.status_code}')
                    break

            except Exception:
                # ç½‘ç»œå¼‚å¸¸é‡è¯•
                if attempt < 2:
                    continue
                self.failed_sub_list.append(url)
                logger.warning(f'{self.mask_url(url)}\tè¯·æ±‚å¤±è´¥ - å·²æ ‡è®°ä¸ºå¤±æ•ˆ')
                break
        
        bar.update(1)

    def start_check_urls(self, url_list):
        logger.info('å¼€å§‹ç­›é€‰---')
        
        # åŠ è½½è‡ªåŠ¨é»‘åå•
        blacklist_set = set()
        if os.path.exists(self.blacklist_path):
            try:
                with open(self.blacklist_path, 'r', encoding='utf-8') as f:
                    lines = f.read().splitlines()
                
                # é™åˆ¶é»‘åå•å¤§å°ï¼Œé˜²æ­¢æ— é™è†¨èƒ€
                blacklist_limit = 50000
                if len(lines) > blacklist_limit:
                    logger.warning(f'é»‘åå•è¡Œæ•° ({len(lines)}) è¶…è¿‡é™åˆ¶ ({blacklist_limit})ï¼Œæ‰§è¡Œè‡ªåŠ¨æ¸…ç†...')
                    # ä¿ç•™æœ€æ–°çš„ 50000 æ¡ (å‡è®¾æ˜¯è¿½åŠ å†™å…¥ï¼Œæœ«å°¾ä¸ºæœ€æ–°)
                    lines = lines[-blacklist_limit:]
                    try:
                        with open(self.blacklist_path, 'w', encoding='utf-8') as f:
                            f.write('\n'.join(lines))
                        logger.info('é»‘åå•æ¸…ç†å®Œæˆ')
                    except Exception as e:
                        logger.error(f'é»‘åå•æ¸…ç†å†™å…¥å¤±è´¥: {e}')

                blacklist_set = set(line.strip() for line in lines if line.strip())
                logger.info(f'å·²åŠ è½½è‡ªåŠ¨é»‘åå•ï¼ŒåŒ…å« {len(blacklist_set)} ä¸ªå¤±æ•ˆé“¾æ¥')
            
            except MemoryError:
                logger.error('åŠ è½½é»‘åå•æ—¶å‘ç”Ÿ MemoryErrorï¼Œæ­£åœ¨é‡ç½®æ–‡ä»¶...')
                try:
                    if os.path.exists(self.blacklist_path):
                        backup_path = self.blacklist_path + '.bak'
                        os.rename(self.blacklist_path, backup_path)
                        logger.warning(f'åŸé»‘åå•å·²å¤‡ä»½è‡³: {backup_path}')
                except Exception as e:
                    logger.error(f'å¤‡ä»½é»‘åå•å¤±è´¥: {e}')
                blacklist_set = set()

            except Exception as e:
                logger.warning(f'åŠ è½½é»‘åå•å¤±è´¥: {e}')

        # é»‘åå•è¿‡æ»¤
        if blacklist_set:
            original_count = len(url_list)
            url_list = [str(url) for url in url_list if str(url) not in blacklist_set]
            filtered_count = original_count - len(url_list)
            if filtered_count > 0:
                logger.info(f'å·²æ ¹æ®é»‘åå•è·³è¿‡ {filtered_count} ä¸ª URL')
        
        bar = tqdm(total=len(url_list), desc='è®¢é˜…ç­›é€‰ï¼š')
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.sub_check, url, bar) for url in url_list]
            concurrent.futures.wait(futures)

        bar.close()
        logger.info('ç­›é€‰å®Œæˆ')

    def save_collected_nodes(self):
        if not self.collected_nodes_set:
            return

        old_nodes = set()
        if os.path.exists(self.collected_nodes_path):
            try:
                with open(self.collected_nodes_path, 'r', encoding='utf-8') as f:
                    old_nodes = set(f.read().splitlines())
            except MemoryError:
                logger.error('è¯»å– collected_nodes.txt æ—¶å‘ç”Ÿ MemoryErrorï¼Œæ­£åœ¨é‡ç½®æ–‡ä»¶...')
                try:
                    backup_path = self.collected_nodes_path + '.bak'
                    os.rename(self.collected_nodes_path, backup_path)
                    logger.warning(f'åŸæ–‡ä»¶å·²å¤‡ä»½è‡³: {backup_path}')
                except Exception as e:
                    logger.error(f'å¤‡ä»½å¤±è´¥: {e}')
                old_nodes = set()
            except Exception as e:
                logger.warning(f'è¯»å–å·²é‡‡é›†èŠ‚ç‚¹å¤±è´¥: {e}')
        
        all_nodes = old_nodes | self.collected_nodes_set
        
        # ä¸¥æ ¼è¿‡æ»¤æ— æ•ˆèŠ‚ç‚¹ (å¿…é¡»åŒ…å« :// ä¸”é•¿åº¦ > 15)
        all_nodes = {node for node in all_nodes if '://' in node and len(node) > 15}

        # é™åˆ¶æ–‡ä»¶å¤§å°
        nodes_limit = 10000
        if len(all_nodes) > nodes_limit:
            logger.info(f'èŠ‚ç‚¹æ€»æ•° ({len(all_nodes)}) è¶…è¿‡é™åˆ¶ ({nodes_limit})ï¼Œæ‰§è¡Œéšæœºé‡‡æ ·æ¸…ç†...')
            # éšæœºä¿ç•™æŒ‡å®šæ•°é‡ï¼Œé˜²æ­¢æ–‡ä»¶è¿‡å¤§
            all_nodes = set(random.sample(list(all_nodes), nodes_limit))
        
        try:
            with open(self.collected_nodes_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sorted(all_nodes)))
            logger.info(f'å·²ä¿å­˜ {len(self.collected_nodes_set)} ä¸ªæ–°èŠ‚ç‚¹åˆ° {self.collected_nodes_path} (å½“å‰æ€»æ•°: {len(all_nodes)})')
        except Exception as e:
            logger.error(f'ä¿å­˜èŠ‚ç‚¹æ–‡ä»¶å¤±è´¥: {e}')

    def sub_update(self, url_list, path_yaml):
        logger.info('å¼€å§‹æ›´æ–°è®¢é˜…---')
        if len(url_list) == 0:
            logger.info('æ²¡æœ‰éœ€è¦æ›´æ–°çš„æ•°æ®')
            return 
        
        # é‡ç½®åˆ—è¡¨
        self.new_sub_list = []
        self.new_clash_list = []
        self.new_v2_list = []
        self.play_list = []
        self.failed_sub_list = []
        
        check_url_list = list(set(url_list))
        
        # å†™å…¥ _url_check.txt
        abs_path_yaml = self.get_abs_path(path_yaml)
        # url_file = abs_path_yaml.replace('.yaml','_url_check.txt')
        # with open(url_file, 'w', encoding='utf-8') as f:
        #     f.write('\n'.join(str(item) for item in check_url_list))
            
        self.start_check_urls(check_url_list)
        
        # å¤„ç†å¤±æ•ˆé“¾æ¥
        if self.failed_sub_list:
            failed_count = len(self.failed_sub_list)
            logger.warning(f'å‘ç° {failed_count} ä¸ªå¤±æ•ˆè®¢é˜…é“¾æ¥ï¼Œå·²è‡ªåŠ¨æ¸…ç†')
            
            timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            with open(self.failed_log_path, 'a', encoding='utf-8') as f:
                f.write(f'\n=== {timestamp} - å¤±æ•ˆè®¢é˜… ({failed_count} ä¸ª) ===\n')
                for failed_url in self.failed_sub_list:
                    f.write(f'{failed_url}\n')
            
            try:
                with open(self.blacklist_path, 'a', encoding='utf-8') as f:
                    for failed_url in self.failed_sub_list:
                        f.write(f'{failed_url}\n')
                logger.info(f'å·²å°† {failed_count} ä¸ªå¤±æ•ˆé“¾æ¥åŠ å…¥è‡ªåŠ¨é»‘åå•')
            except Exception as e:
                logger.warning(f'å†™å…¥é»‘åå•å¤±è´¥: {e}')
        
        # æ›´æ–° YAML
        dict_url = self.load_sub_yaml(path_yaml)
        
        self.new_sub_list = sorted(list(set(self.new_sub_list)))
        self.new_clash_list = sorted(list(set(self.new_clash_list)))
        self.new_v2_list = sorted(list(set(self.new_v2_list)))
        self.play_list = sorted(list(set(self.play_list)))

        dict_url.update({'æœºåœºè®¢é˜…': self.new_sub_list})
        dict_url.update({'clashè®¢é˜…': self.new_clash_list})
        dict_url.update({'v2è®¢é˜…': self.new_v2_list})
        dict_url.update({'å¼€å¿ƒç©è€': self.play_list})
        
        with open(abs_path_yaml, 'w', encoding="utf-8") as f:
            yaml.dump(dict_url, f, allow_unicode=True)
        
        self.print_quality_report()

    def print_quality_report(self):
        total = self.quality_stats['total_checked']
        if total == 0: return
        
        valid_count = len(self.new_sub_list) + len(self.new_clash_list) + len(self.new_v2_list)
        failed_count = len(self.failed_sub_list)
        
        logger.info('='*60)
        logger.info('ğŸ“Š è®¢é˜…æŠ“å–ç»Ÿè®¡æŠ¥å‘Š')
        logger.info('='*60)
        logger.info(f'âœ… æœ‰æ•ˆè®¢é˜…: {valid_count} ä¸ª')
        logger.info(f'   - Clash è®¢é˜…: {len(self.new_clash_list)} ä¸ª')
        logger.info(f'   - V2Ray è®¢é˜…: {len(self.new_v2_list)} ä¸ª')
        logger.info(f'   - æœºåœºè®¢é˜…: {len(self.new_sub_list)} ä¸ª')
        
        if self.enable_quality_check:
            logger.info(f'\nğŸ” è´¨é‡æ§åˆ¶ç»Ÿè®¡:')
            logger.info(f'   - æ£€æŸ¥æ€»æ•°: {total} ä¸ª')
            
            low_quality_total = (self.quality_stats['empty_subscription'] + 
                                self.quality_stats['low_quality'] + 
                                self.quality_stats['spam_content'])
            if low_quality_total > 0:
                logger.info(f'   - ä½è´¨é‡è®¢é˜…: {low_quality_total} ä¸ª')
        
        if failed_count > 0:
            logger.info(f'\nâŒ å¤±æ•ˆè®¢é˜…: {failed_count} ä¸ª')
        logger.info('='*60)

    @logger.catch
    def url_check_valid(self, target, url, bar):
        # æ³¨æ„ï¼šè¿™é‡Œç§»é™¤äº† @retry è£…é¥°å™¨ï¼Œæ”¹ç”±å†…éƒ¨å¾ªç¯å¤„ç†é‡è¯•å’Œæ•…éšœè½¬ç§»
        # è¿™æ ·å¯ä»¥ç¡®ä¿éå†æ‰€æœ‰åç«¯ï¼Œè€Œä¸æ˜¯åªé‡è¯•æŸä¸€ä¸ª
        
        success = False
        url_encode = quote(url, safe='')
        
        # éå†æ‰€æœ‰é…ç½®çš„åç«¯ API
        for api_url in self.check_url_list:
            try:
                check_url_string = self.check_node_url_str.format(api_url, target, url_encode)
                headers = {'User-Agent': self.get_random_ua()}
                
                # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼ŒåŠ å¿«è½®è¯¢é€Ÿåº¦
                res = requests.get(check_url_string, headers=headers, timeout=self.request_timeout, proxies=self.proxies)
                
                if res.status_code == 200:
                    with self.lock:
                        self.airport_list.append(url)
                    success = True
                    break # æˆåŠŸåˆ™åœæ­¢è½®è¯¢
            except requests.RequestException:
                continue # å½“å‰ API å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
            except Exception as e:
                logger.debug(f'è§£æå¤±è´¥: {api_url} - {type(e).__name__}')
                continue
                
        if not success:
            logger.warning(f'æ‰€æœ‰èŠ‚ç‚¹è½¬æ¢ API å‡ä¸å¯ç”¨æˆ–æ£€æµ‹å¤±è´¥: {self.mask_url(url)[:30]}...')
            # å¦‚æœæ˜¯åˆ—è¡¨ä¸ºç©ºå¯¼è‡´æ²¡æœ‰å¾ªç¯ï¼Œä¹Ÿå±äºå¤±è´¥
            if not self.check_url_list:
                logger.warning('æ‰€æœ‰èŠ‚ç‚¹è½¬æ¢ API å‡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶')
        
        bar.update(1)

    def write_url_config(self, url_file, url_list, target):
        logger.info('æ£€æµ‹è®¢é˜…èŠ‚ç‚¹æœ‰æ•ˆæ€§')
        self.airport_list = []
        bar = tqdm(total=len(url_list), desc='èŠ‚ç‚¹æ£€æµ‹ï¼š')
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.url_check_valid, target, url, bar) for url in url_list]
            concurrent.futures.wait(futures)
            
        bar.close()
        logger.info('æ£€æµ‹è®¢é˜…èŠ‚ç‚¹æœ‰æ•ˆæ€§å®Œæˆ')

        # è¯»å–ç›´æ¥é‡‡é›†çš„èŠ‚ç‚¹
        direct_nodes = []
        if os.path.exists(self.collected_nodes_path):
            with open(self.collected_nodes_path, 'r', encoding='utf-8') as f:
                direct_nodes = f.read().splitlines()
        
        # åˆå¹¶æ‰€æœ‰æ¥æº
        final_list = self.airport_list + direct_nodes
        
        # è¿‡æ»¤ï¼šåªä¿ç•™èŠ‚ç‚¹URLï¼Œç§»é™¤è®¢é˜…é“¾æ¥
        nodes_only = []
        for item in final_list:
            item_str = str(item).strip()
            # ä¿ç•™åè®®èŠ‚ç‚¹ï¼Œæ’é™¤httpè®¢é˜…é“¾æ¥
            if '://' in item_str and not item_str.startswith(('http://', 'https://')):
                nodes_only.append(item_str)
        
        # Base64ç¼–ç èŠ‚ç‚¹åˆ—è¡¨
        nodes_text = '\n'.join(nodes_only)
        base64_content = base64.b64encode(nodes_text.encode('utf-8')).decode('utf-8')
        
        # å†™å…¥Base64ç¼–ç çš„è®¢é˜…æ–‡ä»¶
        output_file = url_file.replace('sub_store', target)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(base64_content)
        
        logger.info(f'âœ… å·²ç”Ÿæˆ {target} è®¢é˜…æ–‡ä»¶: {len(nodes_only)} ä¸ªèŠ‚ç‚¹ (Base64ç¼–ç )')

    def write_sub_store(self, yaml_file):
        logger.info('å†™å…¥ sub_store æ–‡ä»¶--')
        dict_url = self.load_sub_yaml(yaml_file)
        abs_yaml_file = self.get_abs_path(yaml_file)

        play_list = dict_url['å¼€å¿ƒç©è€']
        play_url_list = re.findall(self.re_str, str(play_list))
        
        sub_list = dict_url['æœºåœºè®¢é˜…']
        sub_url_list = re.findall(self.re_str, str(sub_list))
        
        write_str = "-- play_list --\n\n\n" + '\n'.join(str(item) for item in play_url_list)
        write_str += "\n\n\n-- sub_list --\n\n\n" + '\n'.join(str(item) for item in sub_url_list)

        url_file = abs_yaml_file.replace('.yaml','_sub_store.txt')
        with open(url_file, 'w', encoding='utf-8') as f:
            f.write(write_str)
        
        self.write_url_config(url_file, play_url_list, 'loon')
        self.write_url_config(url_file, sub_url_list, 'clash')

    def write_merge_files(self, yaml_file):
        """ç”Ÿæˆåˆå¹¶åçš„æ–‡ä»¶"""
        
        # 1. æ±‡æ€»æ‰€æœ‰èŠ‚ç‚¹
        final_nodes = list(self.unique_nodes) # åŒ…å«ä»è®¢é˜…ä¸­è§£æçš„æ‰€æœ‰èŠ‚ç‚¹
        
        # 2. åˆå¹¶ç›´æ¥é‡‡é›†çš„èŠ‚ç‚¹ (è™½ç„¶ sub_check å·²ç»æŠŠè®¢é˜…é‡Œçš„èŠ‚ç‚¹åŠ è¿›å»äº†ï¼Œä½† collected_nodes_set æ¥è‡ªç½‘é¡µçˆ¬å–)
        final_nodes.extend(list(self.collected_nodes_set))
        
        # 3. å†æ¬¡å»é‡å¹¶æ’åº
        final_nodes = sorted(list(set(final_nodes)))
        
        # 4. å†™å…¥ sub_merge.txt (èŠ‚ç‚¹åˆ—è¡¨)
        content_merge = '\n'.join(final_nodes)
        path_merge = os.path.join(self.base_dir, 'sub_merge.txt')
        with open(path_merge, 'w', encoding='utf-8') as f:
            f.write(content_merge)
        
        # 5. å†™å…¥ _url_check.txt (åŒæ ·ä½¿ç”¨å»é‡åçš„èŠ‚ç‚¹é›†åˆï¼Œæ»¡è¶³ç”¨æˆ·éœ€æ±‚)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ yaml_file çš„è·¯å¾„æ¥ç¡®å®š _url_check.txt çš„ä½ç½®ï¼Œæˆ–è€…ç›´æ¥è¦†ç›–
        abs_path_yaml = self.get_abs_path(yaml_file)
        url_check_path = abs_path_yaml.replace('.yaml','_url_check.txt')
        with open(url_check_path, 'w', encoding='utf-8') as f:
            f.write(content_merge)
            
        # 6. å†™å…¥ base64 ç‰ˆæœ¬
        path_base64 = os.path.join(self.base_dir, 'sub_merge_base64.txt')
        with open(path_base64, 'w', encoding='utf-8') as f:
            f.write(base64.b64encode(content_merge.encode('utf-8')).decode('utf-8'))
            
        logger.info(f'åˆå¹¶å®Œæˆ: {len(final_nodes)} ä¸ªå”¯ä¸€èŠ‚ç‚¹å·²å†™å…¥ sub_merge.txt')

        # 6. æ›´æ–° sub_all.yaml (ä»ç„¶ä¿ç•™æœ‰æ•ˆçš„è®¢é˜…é“¾æ¥ä½œä¸ºå†å²è®°å½•)
        # æ³¨æ„ï¼šè¿™é‡Œçš„ new_sub_list ç­‰æ˜¯åœ¨ run() æµç¨‹ä¸­ populated çš„
        # å¦‚æœæ˜¯ merge_sub è°ƒç”¨ sub_updateï¼Œè¿™äº› list åŒ…å«äº†å½“å‰æœ‰æ•ˆçš„æ‰€æœ‰è®¢é˜…
        # æˆ‘ä»¬éœ€è¦è¯»å– yaml_file, ç„¶åæ›´æ–°å®ƒ
        pass # write_sub_store å·²ç»è´Ÿè´£å†™å…¥ yamlï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤å†™å…¥ yaml

    def get_url_form_yaml(self, yaml_file):
        dict_url = self.load_sub_yaml(yaml_file)
        url_list = []
        for key in ['æœºåœºè®¢é˜…', 'clashè®¢é˜…', 'v2è®¢é˜…', 'å¼€å¿ƒç©è€']:
            url_list.extend(dict_url.get(key, []))
        
        url_list = re.findall(self.re_str, str(url_list))
        return [url for url in url_list if self.is_safe_url(url)]

    def get_url_form_channel(self):
        logger.info('è¯»å–configæˆåŠŸ')
        url_list = []
        
        if self.list_tg:
            logger.info(f'å¼€å§‹æŠ“å– {len(self.list_tg)} ä¸ª Telegram é¢‘é“...')
            for channel_url in self.list_tg:
                temp_list = self.fetch_urls_from_page(channel_url)
                if temp_list: url_list.extend(temp_list)
        
        if self.list_web_fuzz:
            logger.info(f'å¼€å§‹æ¨¡ç³ŠæŠ“å– {len(self.list_web_fuzz)} ä¸ªç½‘é¡µ...')
            for web_url in self.list_web_fuzz:
                temp_list = self.fetch_urls_from_page(web_url)
                if temp_list: url_list.extend(temp_list)

        if self.list_subscribe:
            logger.info(f'åŠ è½½ {len(self.list_subscribe)} ä¸ªç›´è¿è®¢é˜…æº...')
            url_list.extend(self.list_subscribe)

        self.save_collected_nodes()
        return url_list

    def run(self):
        start_time = time.time()
        try:
            # 1. Update Today's Sub
            url_list = self.get_url_form_channel()
            path_yaml = pre_check() # pre_check returns relative path
            self.sub_update(url_list, path_yaml)
            
            # 2. Merge Sub
            all_yaml = get_sub_all() # returns relative path
            # pre_check was called above, so path_yaml is valid
            
            merge_url_list = []
            merge_url_list.extend(self.get_url_form_yaml(all_yaml))
            merge_url_list.extend(self.get_url_form_yaml(path_yaml))
            
            self.sub_update(merge_url_list, all_yaml)
            self.write_sub_store(all_yaml)
            self.write_merge_files(all_yaml)
            
            # 3. Notification
            runtime = time.time() - start_time
            runtime_str = f"{int(runtime // 60)}åˆ†{int(runtime % 60)}ç§’"
            
            try:
                from notification import send_notification, format_notification_message
                stats_data = {
                    'valid_count': len(self.new_sub_list) + len(self.new_clash_list) + len(self.new_v2_list),
                    'clash_count': len(self.new_clash_list),
                    'v2ray_count': len(self.new_v2_list),
                    'airport_count': len(self.new_sub_list),
                    'total_checked': self.quality_stats.get('total_checked', 0),
                    'low_quality_count': (self.quality_stats.get('low_quality', 0) + 
                                         self.quality_stats.get('empty_subscription', 0) + 
                                         self.quality_stats.get('spam_content', 0)),
                    'failed_count': len(self.failed_sub_list),
                    'runtime': runtime_str
                }
                message = format_notification_message(stats_data)
                send_notification(message, "SmartSub è¿è¡ŒæˆåŠŸ")
            except Exception as e:
                logger.warning(f'å‘é€é€šçŸ¥å¤±è´¥: {e}')
                
            logger.info('âœ… æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆ')
            
        except Exception as e:
            logger.error(f'âŒ è¿è¡Œå¤±è´¥: {e}')
            try:
                from notification import send_notification, format_error_notification
                error_msg = format_error_notification(str(e))
                send_notification(error_msg, "SmartSub è¿è¡Œå¤±è´¥")
            except:
                pass
            raise

if __name__ == '__main__':
    collector = SubscriptionCollector()
    collector.run()

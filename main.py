# -*- coding: utf-8 -*-
import re
import os
import yaml
import threading
import base64
import json
import requests
import concurrent.futures
import datetime
import time
import random
from loguru import logger
from tqdm import tqdm
from urllib.parse import quote, urlencode, urlparse
from pre_check import pre_check, get_sub_all
from utils import is_safe_url, mask_sensitive_data
from verify_subscription import verify_subscription_file

class SubscriptionCollector:
    def __init__(self):
        # 1. åˆå§‹åŒ–è·¯å¾„ (ä½¿ç”¨ç»å¯¹è·¯å¾„)
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        # åˆ‡æ¢å·¥ä½œç›®å½•åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œç¡®ä¿ pre_check ç­‰å¤–éƒ¨æ¨¡å—èƒ½æ­£ç¡®åˆ›å»ºç›®å½•
        os.chdir(self.base_dir)
        self.config_path = os.path.join(self.base_dir, 'config.yaml')
        self.blacklist_path = os.path.join(self.base_dir, 'blacklist.txt')
        self.collected_nodes_path = os.path.join(self.base_dir, 'collected_nodes.txt')
        self.failed_log_path = os.path.join(self.base_dir, 'failed_subscriptions.log')
        # 2. åˆå§‹åŒ–æ•°æ®å®¹å™¨
        self.new_sub_list = []
        self.new_clash_list = []
        self.new_v2_list = []
        self.play_list = []
        self.airport_list = []
        self.collected_nodes_set = set()
        self.failed_sub_list = []
        self.failed_sub_reasons = {}
        self.low_quality_sub_reasons = {}
        # 3. è´¨é‡æ§åˆ¶ä¸ç»Ÿè®¡
        self.quality_stats = {
            'total_checked': 0,
            'low_quality': 0,
            'empty_subscription': 0,
            'spam_content': 0
        }
        self.lock = threading.Lock()
        # 4. æ­£åˆ™è¡¨è¾¾å¼
        self.re_str = r"https?://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]"
        # æ›´å…¨é¢çš„èŠ‚ç‚¹ URL æ­£åˆ™ï¼ˆå…¼å®¹ RFC 3986ï¼Œé˜²æ­¢å‚æ•°æˆªæ–­ï¼‰
        self.node_str = r'(?:vmess|ss|trojan|vless|hysteria2)://[-a-zA-Z0-9+/=@#?&._%[\]:~!*();,]+'
        self.check_node_url_str = "https://{}/sub?target={}&url={}&insert=false&config=config%2FACL4SSR.ini"
        # 5. é…ç½®å‚æ•° (é»˜è®¤å€¼)
        self.max_workers = 32
        self.content_limit_mb = 3
        self.request_timeout = 15
        self.min_nodes = 3
        self.enable_quality_check = True
        self.check_url_list = []
        # 6. User-Agent åˆ—è¡¨ (æŠ—å°é” - æ‰©å±•æ± )
        self.user_agents = [
            # Chrome
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            # Edge
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/124.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Edg/124.0.0.0 Safari/537.36",
            # Firefox
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:125.0) Gecko/20100101 Firefox/125.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:125.0) Gecko/20100101 Firefox/125.0",
            # Safari
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Safari/605.1.15",
            # Mobile
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Mobile/15E148 Safari/604.1",
            "Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Mobile Safari/537.36"
        ]
        # 7. é™æ€èµ„æºåç¼€ (ç”¨äºè¿‡æ»¤æ— æ•ˆé“¾æ¥)
        self.static_extensions = (
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.ico', '.svg', 
            '.css', '.js', '.woff', '.woff2', '.ttf', '.eot', '.otf',
            '.mp3', '.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv',
            '.zip', '.rar', '.7z', '.tar', '.gz', '.iso', '.dmg', '.exe', '.apk'
        )
        # 8. ä»£ç†é…ç½® (æ”¯æŒ GitHub Actions ç­‰ç¯å¢ƒ)
        self.proxies = self._get_system_proxies()
        self.list_tg = []
        self.list_subscribe = []
        self.list_web_fuzz = []
        # åŠ è½½é…ç½®
        self.load_config()
    def _extract_github_user(self, url):
        if not url:
            return None
        url_lower = url.lower()
        for marker in ['raw.githubusercontent.com/', 'gist.githubusercontent.com/', 'github.com/']:
            if marker in url_lower:
                tail = url_lower.split(marker, 1)[1]
                user = tail.split('/', 1)[0]
                return user if user else None
        try:
            parsed = urlparse(url)
            host = parsed.netloc.lower()
            if host.endswith('github.com'):
                segments = [s for s in parsed.path.split('/') if s]
                if segments:
                    return segments[0].lower()
        except Exception:
            return None
        return None
    def _dedupe_github_users(self, url_list):
        if not url_list:
            return url_list
        seen_users = set()
        deduped = []
        removed = 0
        for url in url_list:
            user = self._extract_github_user(url)
            if user:
                if user in seen_users:
                    removed += 1
                    continue
                seen_users.add(user)
            deduped.append(url)
        if removed:
            logger.info(f'GitHub ç”¨æˆ·å»é‡: ç§»é™¤ {removed} æ¡é‡å¤è®¢é˜…é“¾æ¥')
        return deduped
    def _record_failed(self, url, reason):
        if not url:
            return
        if url not in self.failed_sub_reasons:
            self.failed_sub_reasons[url] = reason
            self.failed_sub_list.append(url)
    def _record_low_quality(self, url, reason):
        if not url:
            return
        if url not in self.low_quality_sub_reasons:
            self.low_quality_sub_reasons[url] = reason
    def get_abs_path(self, relative_path):
        """å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºåŸºäºè„šæœ¬ç›®å½•çš„ç»å¯¹è·¯å¾„"""
        if os.path.isabs(relative_path):
            return relative_path
        return os.path.join(self.base_dir, relative_path)
    @logger.catch
    def load_config(self):
        if not os.path.exists(self.config_path):
            logger.error(f"Config file not found: {self.config_path}")
            return
        with open(self.config_path, encoding="UTF-8") as f:
            data = yaml.safe_load(f)
        # è¯»å–æ€§èƒ½é…ç½®
        performance = data.get('performance', {})
        self.max_workers = performance.get('max_workers', 32)
        self.content_limit_mb = performance.get('content_limit_mb', 3)
        self.request_timeout = performance.get('request_timeout', 15)
        # éªŒè¯é…ç½®å‚æ•°èŒƒå›´
        try:
            assert 1 <= self.max_workers <= 128, f"max_workers å¿…é¡»åœ¨ 1-128 ä¹‹é—´ï¼Œå½“å‰: {self.max_workers}"
            assert 1 <= self.content_limit_mb <= 50, f"content_limit_mb å¿…é¡»åœ¨ 1-50 ä¹‹é—´ï¼Œå½“å‰: {self.content_limit_mb}"
            assert 3 <= self.request_timeout <= 60, f"request_timeout å¿…é¡»åœ¨ 3-60 ä¹‹é—´ï¼Œå½“å‰: {self.request_timeout}"
        except AssertionError as e:
            logger.error(f"âŒ é…ç½®å‚æ•°é”™è¯¯: {e}")
            raise
        # è¯»å–è´¨é‡æ§åˆ¶é…ç½®
        quality = data.get('quality_control', {})
        self.min_nodes = quality.get('min_nodes', 3)
        self.enable_quality_check = quality.get('enable_quality_check', True)
        # éªŒè¯è´¨é‡æ§åˆ¶å‚æ•°
        try:
            assert 1 <= self.min_nodes <= 100, f"min_nodes å¿…é¡»åœ¨ 1-100 ä¹‹é—´ï¼Œå½“å‰: {self.min_nodes}"
        except AssertionError as e:
            logger.error(f"âŒ é…ç½®å‚æ•°é”™è¯¯: {e}")
            raise
        # èŠ‚ç‚¹çº§å»é‡æ± 
        self.unique_nodes = set()
        logger.info(f'âœ… æ€§èƒ½é…ç½®: çº¿ç¨‹æ•°={self.max_workers}, é™åˆ¶={self.content_limit_mb}MB, è¶…æ—¶={self.request_timeout}s')
        logger.info(f'âœ… è´¨é‡æ§åˆ¶: æœ€å°‘èŠ‚ç‚¹={self.min_nodes}, è´¨æ£€={self.enable_quality_check}')
        # è·å– Telegram é¢‘é“
        list_tg_raw = data.get('tgchannel', [])
        self.list_tg = []
        for url in list_tg_raw:
            url = str(url).strip()
            if not url:
                continue
            # ä½¿ç”¨æ­£åˆ™æ™ºèƒ½æå–é¢‘é“ ID
            # åŒ¹é…: t.me/channel, t.me/s/channel, telegram.me/channel
            # èƒ½å¤Ÿå¤„ç†æœ«å°¾æ–œæ ã€å‚æ•°ç­‰æƒ…å†µ
            match = re.search(r'(?:t\.me|telegram\.me)/(?:s/)?([a-zA-Z0-9_]+)', url, re.IGNORECASE)
            if match:
                channel_id = match.group(1)
                # æ’é™¤ä¸€äº›éé¢‘é“çš„ç³»ç»Ÿè·¯å¾„
                if channel_id.lower() not in ['s', 'share', 'joinchat', 'addstickers', 'iv']:
                    self.list_tg.append(f'https://t.me/s/{channel_id}')
            elif '/' not in url and '@' not in url:
                # æ”¯æŒçº¯é¢‘é“å: channel_name
                self.list_tg.append(f'https://t.me/s/{url}')
            elif url.startswith('@'):
                # æ”¯æŒ @channel_name
                self.list_tg.append(f'https://t.me/s/{url[1:]}')
            else:
                logger.warning(f'å¿½ç•¥æ— æ³•è§£æçš„ Telegram é“¾æ¥: {url}')
        self.list_subscribe = data.get('subscribe', [])
        self.list_web_fuzz = data.get('web_pages', [])
        # è·å–è®¢é˜…è½¬æ¢ API
        # ä¼˜å…ˆè¯»å– subconverter_backendsï¼Œå…¼å®¹æ—§é…ç½® sub_convert_apis
        config_apis = data.get('subconverter_backends') or data.get('sub_convert_apis', [])
        if config_apis:
            self.check_url_list = config_apis
            logger.info(f'å·²åŠ è½½ {len(self.check_url_list)} ä¸ªè®¢é˜…è½¬æ¢ API')
        else:
            logger.warning('æœªé…ç½® subconverter_backendsï¼Œå°†ä½¿ç”¨é»˜è®¤ API')
            # æä¾›ä¸€ç»„å†…ç½®çš„é»˜è®¤ API é˜²æ­¢ç¨‹åºå‡ºé”™
            self.check_url_list = ['api.dler.io','sub.xeton.dev','sub.id9.cc','sub.maoxiongnet.com']
    @logger.catch
    def load_sub_yaml(self, path_yaml):
        abs_path = self.get_abs_path(path_yaml)
        if os.path.isfile(abs_path):
            with open(abs_path, encoding="UTF-8") as f:
                dict_url = yaml.safe_load(f)
        else:
            dict_url = {
                "æœºåœºè®¢é˜…": [],
                "clashè®¢é˜…": [],
                "v2è®¢é˜…": [],
                "å¼€å¿ƒç©è€": []
            }
        logger.info(f'è¯»å–æ–‡ä»¶æˆåŠŸ: {abs_path}')
        return dict_url
    def get_random_ua(self):
        """éšæœºè·å– User-Agent"""
        return random.choice(self.user_agents)
    def check_ssrf(self, url):
        """ç®€å•çš„ SSRF é˜²å¾¡æ£€æµ‹"""
        if not url: return False
        try:
            url_lower = url.lower()
            # ç®€å•åˆ¤æ–­æ˜¯å¦ä»¥ localhost æˆ– 127.0.0.1 å¼€å¤´
            if url_lower.startswith(('http://localhost', 'https://localhost', 
                                   'http://127.0.0.1', 'https://127.0.0.1')):
                logger.warning(f'æ‹¦æˆªæ½œåœ¨çš„ SSRF è¯·æ±‚: {mask_sensitive_data(url)}')
                return False
            return True
        except Exception:
            return False
    def _get_system_proxies(self):
        """ä»ç¯å¢ƒå˜é‡è·å–ä»£ç†è®¾ç½®"""
        proxies = {}
        http_proxy = os.environ.get('HTTP_PROXY') or os.environ.get('http_proxy')
        https_proxy = os.environ.get('HTTPS_PROXY') or os.environ.get('https_proxy')
        if http_proxy:
            proxies['http'] = http_proxy
        if https_proxy:
            proxies['https'] = https_proxy
        if proxies:
            logger.info(f'å·²æ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†è®¾ç½®: {proxies}')
        return proxies if proxies else None
    @logger.catch
    def fetch_urls_from_page(self, url):
        """é€šç”¨ç½‘é¡µæŠ“å–å‡½æ•° (å¢å¼ºæŠ—å°é”)"""
        if not self.check_ssrf(url):
            return []
        # é’ˆå¯¹ Telegram é¢‘é“çš„ä¼˜åŒ–ï¼šä¸é‡è¯•ï¼Œå¿«é€Ÿè·³è¿‡
        is_tg_channel = 't.me/s/' in url
        url_list = []
        node_list = []
        data = None
        try:
            headers = {
                'User-Agent': self.get_random_ua()
            }
            # å‘èµ·è¯·æ±‚ (å¯ç”¨ stream æ¨¡å¼é˜²æ­¢å†…å­˜æº¢å‡º)
            resp = requests.get(url, headers=headers, timeout=self.request_timeout, proxies=self.proxies, stream=True)
            # ä¸¥æ ¼è¿‡æ»¤ï¼šæ£€æµ‹ 400 ä»¥ä¸Šçš„é¡µé¢ç›´æ¥è·³è¿‡ (ç”¨æˆ·æŒ‡ä»¤)
            if resp.status_code >= 400:
                resp.close()
                logger.warning(f'{mask_sensitive_data(url)}\tçŠ¶æ€ç  {resp.status_code} >= 400ï¼Œç›´æ¥è·³è¿‡')
                return []
            # æ­£å¸¸å“åº”å¤„ç†
            content_limit = self.content_limit_mb * 1024 * 1024
            content = b""
            try:
                for chunk in resp.iter_content(chunk_size=8192):
                    content += chunk
                    if len(content) > content_limit:
                        logger.warning(f'{mask_sensitive_data(url)}\tè¶…è¿‡å¤§å°é™åˆ¶({self.content_limit_mb}MB)ï¼Œæˆªæ–­ä¸‹è½½')
                        # ä¸éœ€è¦æ ‡è®°ä¸ºå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨æˆªæ–­åçš„å†…å®¹å°è¯•æå–
                        break
            except Exception as e:
                logger.warning(f'{mask_sensitive_data(url)}\tä¸‹è½½ä¸­æ–­: {e}')
            resp.close()
            # å°è¯•è§£ç 
            data = content.decode('utf-8', errors='ignore')
        except requests.RequestException as e:
            if not is_tg_channel:
                logger.warning(f'{mask_sensitive_data(url)}\tç½‘ç»œè¯·æ±‚å¤±è´¥: {type(e).__name__}')
            return []
        except Exception as e:
            logger.error(f'{mask_sensitive_data(url)}\tå¤„ç†å¤±è´¥: {type(e).__name__} - {str(e)}')
            return []
        if not data:
            return []
        try:
            # 1. æå–è®¢é˜… URL
            all_url_list = re.findall(self.re_str, data)
            filter_string_list = ["//t.me/", "cdn-telegram.org", "w3.org", "google.com", "github.com/site", "github.com/features", "cdn5.telesco.pe"]
            url_list = [item for item in all_url_list if not any(filter_string in item for filter_string in filter_string_list)]
            # è¿‡æ»¤é™æ€èµ„æº
            url_list = [item for item in url_list if not item.lower().endswith(self.static_extensions)]
            url_list = list(set(url_list))
            # è¿‡æ»¤æ•æ„Ÿé“¾æ¥
            url_list = [u for u in url_list if is_safe_url(u)]
            # 2. æå–ç›´æ¥èŠ‚ç‚¹
            direct_nodes = re.findall(self.node_str, data)
            if direct_nodes:
                node_list.extend(direct_nodes)
                logger.info(f'{mask_sensitive_data(url)}\tå‘ç° {len(direct_nodes)} ä¸ªç›´æ¥èŠ‚ç‚¹')
            if node_list:
                self.collected_nodes_set.update(node_list)
            # 3. è´¨é‡æ§åˆ¶
            if len(url_list) == 0 and len(node_list) == 0:
                logger.warning(f'{mask_sensitive_data(url)}\tæ— æœ‰æ•ˆå†…å®¹')
                return []
            if len(url_list) + len(node_list) < 2:
                logger.warning(f'{mask_sensitive_data(url)}\tå†…å®¹è¿‡å°‘({len(url_list) + len(node_list)} < 2)ï¼Œå·²è·³è¿‡')
                return []
            logger.info(f'{mask_sensitive_data(url)}\tè·å–æˆåŠŸ\tè®¢é˜…é“¾æ¥:{len(url_list)} èŠ‚ç‚¹é“¾æ¥:{len(node_list)}')
        except Exception as e:
            logger.error(f'{mask_sensitive_data(url)}\tæ•°æ®è§£æå¤±è´¥: {type(e).__name__} - {str(e)}')
        return url_list
    def filter_base64(self, text):
        ss = ['ss://', 'vmess://', 'trojan://', 'vless://', 'hysteria2://']
        for i in ss:
            if i in text:
                return True
        return False
    def extract_nodes(self, content):
        """ä»å†…å®¹ä¸­æå–èŠ‚ç‚¹é“¾æ¥"""
        nodes = []
        try:
            # å°è¯•è§£æ Base64
            decoded_text = ""
            try:
                # ç®€å•çš„ Base64 æ¢æµ‹
                sample_length = min(256, len(content))
                head_text = content[:sample_length].strip()
                if not '://' in head_text and not 'proxies:' in head_text: # åªæœ‰ä¸åŒ…å«åè®®å¤´æ‰å°è¯•è§£ç 
                    missing_padding = len(content) % 4
                    if missing_padding:
                        content += '=' * (4 - missing_padding)
                    decoded_text = base64.b64decode(content).decode('utf-8', errors='ignore')
            except Exception:
                pass
            # ä»åŸå§‹å†…å®¹æå–
            nodes.extend(re.findall(self.node_str, content))
            # ä»è§£ç å†…å®¹æå–
            if decoded_text:
                nodes.extend(re.findall(self.node_str, decoded_text))
        except Exception:
            pass
        return list(set(nodes)) # å±€éƒ¨å»é‡
    def count_nodes_in_content(self, content, is_clash=False):
        try:
            if is_clash:
                data = yaml.safe_load(content)
                proxies = data.get('proxies', [])
                return len(proxies)
            else:
                try:
                    decoded = base64.b64decode(content).decode('utf-8', errors='ignore')
                    nodes = [line for line in decoded.split('\n') if line.strip() and '://' in line]
                    return len(nodes)
                except Exception:
                    return 0
        except Exception:
            return 0
    def validate_subscription_quality(self, url, content, is_clash=False):
        if not self.enable_quality_check:
            return True
        node_count = self.count_nodes_in_content(content, is_clash)
        if node_count == 0:
            logger.warning(f'{mask_sensitive_data(url)}\tç©ºè®¢é˜…ï¼ˆ0ä¸ªèŠ‚ç‚¹ï¼‰- å·²è·³è¿‡')
            with self.lock:
                self.quality_stats['empty_subscription'] += 1
            self._record_low_quality(url, 'empty_subscription')
            return False
        if node_count < self.min_nodes:
            logger.warning(f'{mask_sensitive_data(url)}\tèŠ‚ç‚¹è¿‡å°‘ï¼ˆ{node_count} < {self.min_nodes}ï¼‰- å·²è·³è¿‡')
            with self.lock:
                self.quality_stats['low_quality'] += 1
            self._record_low_quality(url, 'low_nodes')
            return False
        spam_keywords = ['å·²è¿‡æœŸ', 'è¯·è´­ä¹°', 'è¯•ç”¨ç»“æŸ', 'è”ç³»å®¢æœ', 'å·²åˆ°æœŸ']
        content_lower = content.lower()
        if any(keyword in content_lower for keyword in spam_keywords):
            logger.warning(f'{mask_sensitive_data(url)}\tæ£€æµ‹åˆ°åƒåœ¾å†…å®¹ - å·²è·³è¿‡')
            with self.lock:
                self.quality_stats['spam_content'] += 1
            self._record_low_quality(url, 'spam_content')
            return False
        logger.info(f'{mask_sensitive_data(url)}\tè´¨é‡éªŒè¯é€šè¿‡ï¼ˆ{node_count} ä¸ªèŠ‚ç‚¹ï¼‰')
        return True
    @logger.catch
    def sub_check(self, url, bar):
        if not self.check_ssrf(url):
            bar.update(1)
            return
        # å¿«é€Ÿè¿‡æ»¤é™æ€èµ„æºåç¼€ (äºŒæ¬¡ä¿é™©)
        if url.lower().endswith(self.static_extensions):
            bar.update(1)
            return
        # å•æ¬¡è¯·æ±‚ï¼Œä¸é‡è¯•
        try:
            headers = {'User-Agent': self.get_random_ua()}
            res = requests.get(url, headers=headers, timeout=self.request_timeout, stream=True, proxies=self.proxies)
            # ä¸¥æ ¼è¿‡æ»¤ï¼šæ£€æµ‹ 400 ä»¥ä¸Šçš„é¡µé¢ç›´æ¥è·³è¿‡ (ç”¨æˆ·æŒ‡ä»¤)
            if res.status_code >= 400:
                res.close()
                self._record_failed(url, f'http_{res.status_code}')
                logger.warning(f'{mask_sensitive_data(url)}\tçŠ¶æ€ç  {res.status_code} >= 400ï¼Œç›´æ¥è·³è¿‡')
                bar.update(1)
                return
            if res.status_code == 200:
                header_info_valid = False
                header_play_info = ""
                # Header Check
                # æ³¨æ„ï¼šè·å–åˆ°æµé‡ä¿¡æ¯åä¸åº”ç›´æ¥è¿”å›ï¼Œå¿…é¡»ç»§ç»­æ‰§è¡Œ Body ä¸‹è½½å’ŒèŠ‚ç‚¹æå–ï¼Œ
                # è¿™æ ·æ‰èƒ½ç¡®ä¿è¯¥è®¢é˜…ä¸­çš„èŠ‚ç‚¹è¢«è§£æå¹¶åŠ å…¥å»é‡æ± ã€‚
                try:
                    info = res.headers.get('subscription-userinfo')
                    if info:
                        info_num = re.findall(r'\d+', info)
                        if info_num:
                            upload = int(info_num[0])
                            download = int(info_num[1])
                            total = int(info_num[2])
                            unused = (total - upload - download) / 1024 / 1024 / 1024
                            unused_rounded = round(unused, 2)
                            if unused_rounded > 0:
                                header_info_valid = True
                                header_play_info = 'å¯ç”¨æµé‡:' + str(unused_rounded) + ' GB                    ' + url
                except Exception:
                    pass
                # Body Check
                content_limit = self.content_limit_mb * 1024 * 1024
                content = b""
                try:
                    for chunk in res.iter_content(chunk_size=8192):
                        content += chunk
                        if len(content) > content_limit:
                            logger.debug(f'{mask_sensitive_data(url)} è¶…è¿‡å¤§å°é™åˆ¶ï¼Œæˆªæ–­ä¸‹è½½')
                            break
                    text = content.decode('utf-8', errors='ignore')
                except Exception:
                    res.close()
                    self._record_failed(url, 'download_failed')
                    logger.warning(f'{mask_sensitive_data(url)}\tä¸‹è½½ä¸­æ–­ - å·²æ ‡è®°ä¸ºå¤±æ•ˆ')
                    bar.update(1)
                    return
                finally:
                    res.close()
                # è´¨é‡æ§åˆ¶ï¼šå†…å®¹å»é‡æ£€æŸ¥ (å·²åºŸå¼ƒæ–‡ä»¶çº§ MD5 å»é‡)
                with self.lock:
                    self.quality_stats['total_checked'] += 1
                # è§£æèŠ‚ç‚¹å¹¶åŠ å…¥å…¨å±€å»é‡æ± 
                nodes = self.extract_nodes(text)
                if nodes:
                    with self.lock:
                        self.unique_nodes.update(nodes)
                # Clash åˆ¤æ–­
                try:
                    if 'proxies:' in text:
                        if not self.validate_subscription_quality(url, text, is_clash=True):
                            bar.update(1)
                            return
                        with self.lock:
                            self.new_clash_list.append(url)
                            if header_info_valid:
                                self.new_sub_list.append(url)
                                self.play_list.append(header_play_info)
                        bar.update(1)
                        return
                except Exception:
                    pass
                # V2Ray/Base64 åˆ¤æ–­
                try:
                    sample_length = min(256, len(text))
                    head_text = text[:sample_length].strip()
                    missing_padding = len(head_text) % 4
                    if missing_padding:
                        head_text += '=' * (4 - missing_padding)
                    decoded_text = base64.b64decode(head_text).decode('utf-8', errors='ignore')
                    if self.filter_base64(decoded_text):
                        if not self.validate_subscription_quality(url, text, is_clash=False):
                            bar.update(1)
                            return
                        with self.lock:
                            self.new_v2_list.append(url)
                            if header_info_valid:
                                self.new_sub_list.append(url)
                                self.play_list.append(header_play_info)
                except Exception:
                    pass
                bar.update(1)
                return
            # é 200 ä¹Ÿé >= 400 çš„å…¶ä»–çŠ¶æ€ (å¦‚ 3xx æœªè·³è½¬)
            res.close()
            self._record_failed(url, f'http_{res.status_code}')
            logger.warning(f'{mask_sensitive_data(url)}\tçŠ¶æ€ç å¼‚å¸¸: {res.status_code}')
            bar.update(1)
            return
        except Exception:
            self._record_failed(url, 'request_failed')
            logger.warning(f'{mask_sensitive_data(url)}\tè¯·æ±‚å¤±è´¥ - å·²æ ‡è®°ä¸ºå¤±æ•ˆ')
            bar.update(1)
            return
    def start_check_urls(self, url_list):
        logger.info('å¼€å§‹ç­›é€‰---')
        # åŠ è½½è‡ªåŠ¨é»‘åå•
        blacklist_set = set()
        if os.path.exists(self.blacklist_path):
            try:
                with open(self.blacklist_path, 'r', encoding='utf-8') as f:
                    lines = f.read().splitlines()
                # é™åˆ¶é»‘åå•å¤§å°ï¼Œé˜²æ­¢æ— é™è†¨èƒ€
                blacklist_limit = 50000
                if len(lines) > blacklist_limit:
                    logger.warning(f'é»‘åå•è¡Œæ•° ({len(lines)}) è¶…è¿‡é™åˆ¶ ({blacklist_limit})ï¼Œæ‰§è¡Œè‡ªåŠ¨æ¸…ç†...')
                    # ä¿ç•™æœ€æ–°çš„ 50000 æ¡ (å‡è®¾æ˜¯è¿½åŠ å†™å…¥ï¼Œæœ«å°¾ä¸ºæœ€æ–°)
                    lines = lines[-blacklist_limit:]
                    try:
                        with open(self.blacklist_path, 'w', encoding='utf-8') as f:
                            f.write('\n'.join(lines))
                        logger.info('é»‘åå•æ¸…ç†å®Œæˆ')
                    except Exception as e:
                        logger.error(f'é»‘åå•æ¸…ç†å†™å…¥å¤±è´¥: {e}')
                blacklist_set = set(line.strip() for line in lines if line.strip())
                logger.info(f'å·²åŠ è½½è‡ªåŠ¨é»‘åå•ï¼ŒåŒ…å« {len(blacklist_set)} ä¸ªå¤±æ•ˆé“¾æ¥')
            except MemoryError:
                logger.error('åŠ è½½é»‘åå•æ—¶å‘ç”Ÿ MemoryErrorï¼Œæ­£åœ¨é‡ç½®æ–‡ä»¶...')
                try:
                    if os.path.exists(self.blacklist_path):
                        backup_path = self.blacklist_path + '.bak'
                        os.rename(self.blacklist_path, backup_path)
                        logger.warning(f'åŸé»‘åå•å·²å¤‡ä»½è‡³: {backup_path}')
                except Exception as e:
                    logger.error(f'å¤‡ä»½é»‘åå•å¤±è´¥: {e}')
                blacklist_set = set()
            except Exception as e:
                logger.warning(f'åŠ è½½é»‘åå•å¤±è´¥: {e}')
        # é»‘åå•è¿‡æ»¤
        if blacklist_set:
            original_count = len(url_list)
            url_list = [str(url) for url in url_list if str(url) not in blacklist_set]
            filtered_count = original_count - len(url_list)
            if filtered_count > 0:
                logger.info(f'å·²æ ¹æ®é»‘åå•è·³è¿‡ {filtered_count} ä¸ª URL')
        bar = tqdm(total=len(url_list), desc='è®¢é˜…ç­›é€‰ï¼š')
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.sub_check, url, bar) for url in url_list]
            concurrent.futures.wait(futures)
        bar.close()
        logger.info('ç­›é€‰å®Œæˆ')
    def save_collected_nodes(self):
        if not self.collected_nodes_set:
            return
        old_nodes = set()
        if os.path.exists(self.collected_nodes_path):
            try:
                with open(self.collected_nodes_path, 'r', encoding='utf-8') as f:
                    old_nodes = set(f.read().splitlines())
            except MemoryError:
                logger.error('è¯»å– collected_nodes.txt æ—¶å‘ç”Ÿ MemoryErrorï¼Œæ­£åœ¨é‡ç½®æ–‡ä»¶...')
                try:
                    backup_path = self.collected_nodes_path + '.bak'
                    os.rename(self.collected_nodes_path, backup_path)
                    logger.warning(f'åŸæ–‡ä»¶å·²å¤‡ä»½è‡³: {backup_path}')
                except Exception as e:
                    logger.error(f'å¤‡ä»½å¤±è´¥: {e}')
                old_nodes = set()
            except Exception as e:
                logger.warning(f'è¯»å–å·²é‡‡é›†èŠ‚ç‚¹å¤±è´¥: {e}')
        all_nodes = old_nodes | self.collected_nodes_set
        # ä¸¥æ ¼è¿‡æ»¤æ— æ•ˆèŠ‚ç‚¹ (å¿…é¡»åŒ…å« :// ä¸”é•¿åº¦ > 15)
        all_nodes = {node for node in all_nodes if '://' in node and len(node) > 15}
        # é™åˆ¶æ–‡ä»¶å¤§å°
        nodes_limit = 10000
        if len(all_nodes) > nodes_limit:
            logger.info(f'èŠ‚ç‚¹æ€»æ•° ({len(all_nodes)}) è¶…è¿‡é™åˆ¶ ({nodes_limit})ï¼Œæ‰§è¡Œéšæœºé‡‡æ ·æ¸…ç†...')
            # éšæœºä¿ç•™æŒ‡å®šæ•°é‡ï¼Œé˜²æ­¢æ–‡ä»¶è¿‡å¤§
            all_nodes = set(random.sample(list(all_nodes), nodes_limit))
        try:
            with open(self.collected_nodes_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sorted(all_nodes)))
            logger.info(f'å·²ä¿å­˜ {len(self.collected_nodes_set)} ä¸ªæ–°èŠ‚ç‚¹åˆ° {self.collected_nodes_path} (å½“å‰æ€»æ•°: {len(all_nodes)})')
        except Exception as e:
            logger.error(f'ä¿å­˜èŠ‚ç‚¹æ–‡ä»¶å¤±è´¥: {e}')
    def sub_update(self, url_list, path_yaml):
        logger.info('å¼€å§‹æ›´æ–°è®¢é˜…---')
        if len(url_list) == 0:
            logger.info('æ²¡æœ‰éœ€è¦æ›´æ–°çš„æ•°æ®')
            return 
        # é‡ç½®åˆ—è¡¨
        self.new_sub_list = []
        self.new_clash_list = []
        self.new_v2_list = []
        self.play_list = []
        self.failed_sub_list = []
        self.failed_sub_reasons = {}
        self.low_quality_sub_reasons = {}
        check_url_list = list(dict.fromkeys(url_list))
        check_url_list = self._dedupe_github_users(check_url_list)
        # å†™å…¥ _url_check.txt
        abs_path_yaml = self.get_abs_path(path_yaml)
        # url_file = abs_path_yaml.replace('.yaml','_url_check.txt')
        # with open(url_file, 'w', encoding='utf-8') as f:
        #     f.write('\n'.join(str(item) for item in check_url_list))
        self.start_check_urls(check_url_list)
        # å¤„ç†å¤±æ•ˆé“¾æ¥
        if self.failed_sub_list:
            failed_count = len(self.failed_sub_list)
            logger.warning(f'å‘ç° {failed_count} ä¸ªå¤±æ•ˆè®¢é˜…é“¾æ¥ï¼Œå·²è‡ªåŠ¨æ¸…ç†')
            # æ—¥å¿—å¤§å°é™åˆ¶å’Œè½®è½¬ï¼ˆé˜²æ­¢æ— é™å¢é•¿ï¼‰
            max_log_size = 1024 * 1024  # 1MB
            if os.path.exists(self.failed_log_path):
                log_size = os.path.getsize(self.failed_log_path)
                if log_size > max_log_size:
                    backup_path = self.failed_log_path + '.old'
                    try:
                        os.rename(self.failed_log_path, backup_path)
                        logger.info(f'æ—¥å¿—æ–‡ä»¶è¿‡å¤§ ({log_size/1024/1024:.2f}MB)ï¼Œå·²å¤‡ä»½åˆ° {backup_path}')
                    except Exception as e:
                        logger.warning(f'æ—¥å¿—å¤‡ä»½å¤±è´¥: {e}')
            timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            with open(self.failed_log_path, 'a', encoding='utf-8') as f:
                f.write(f'\n=== {timestamp} - å¤±æ•ˆè®¢é˜… ({failed_count} ä¸ª) ===\n')
                for failed_url in self.failed_sub_list:
                    reason = self.failed_sub_reasons.get(failed_url, 'unknown')
                    f.write(f'{failed_url}\t{reason}\n')
            try:
                with open(self.blacklist_path, 'a', encoding='utf-8') as f:
                    for failed_url in self.failed_sub_list:
                        f.write(f'{failed_url}\n')
                logger.info(f'å·²å°† {failed_count} ä¸ªå¤±æ•ˆé“¾æ¥åŠ å…¥è‡ªåŠ¨é»‘åå•')
            except Exception as e:
                logger.warning(f'å†™å…¥é»‘åå•å¤±è´¥: {e}')
        # æ›´æ–° YAML
        dict_url = self.load_sub_yaml(path_yaml)
        self.new_sub_list = sorted(list(set(self.new_sub_list)))
        self.new_clash_list = sorted(list(set(self.new_clash_list)))
        self.new_v2_list = sorted(list(set(self.new_v2_list)))
        self.play_list = sorted(list(set(self.play_list)))
        dict_url.update({'æœºåœºè®¢é˜…': self.new_sub_list})
        dict_url.update({'clashè®¢é˜…': self.new_clash_list})
        dict_url.update({'v2è®¢é˜…': self.new_v2_list})
        dict_url.update({'å¼€å¿ƒç©è€': self.play_list})
        with open(abs_path_yaml, 'w', encoding="utf-8") as f:
            yaml.dump(dict_url, f, allow_unicode=True)
        self.save_source_health(path_yaml, check_url_list)
        self.print_quality_report()
    def save_source_health(self, path_yaml, check_url_list):
        if not self.failed_sub_reasons and not self.low_quality_sub_reasons:
            return
        try:
            runtime_dir = os.path.join(self.base_dir, 'runtime')
            os.makedirs(runtime_dir, exist_ok=True)
            output_path = os.path.join(runtime_dir, 'source_health.json')
            payload = {
                'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'target': path_yaml,
                'total_checked': len(check_url_list),
                'failed': [
                    {'url': url, 'reason': reason}
                    for url, reason in self.failed_sub_reasons.items()
                ],
                'low_quality': [
                    {'url': url, 'reason': reason}
                    for url, reason in self.low_quality_sub_reasons.items()
                ]
            }
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f'å†™å…¥ source_health.json å¤±è´¥: {e}')
    def print_quality_report(self):
        total = self.quality_stats['total_checked']
        if total == 0: return
        valid_count = len(self.new_sub_list) + len(self.new_clash_list) + len(self.new_v2_list)
        failed_count = len(self.failed_sub_list)
        logger.info('='*60)
        logger.info('ğŸ“Š è®¢é˜…æŠ“å–ç»Ÿè®¡æŠ¥å‘Š')
        logger.info('='*60)
        logger.info(f'âœ… æœ‰æ•ˆè®¢é˜…: {valid_count} ä¸ª')
        logger.info(f'   - Clash è®¢é˜…: {len(self.new_clash_list)} ä¸ª')
        logger.info(f'   - V2Ray è®¢é˜…: {len(self.new_v2_list)} ä¸ª')
        logger.info(f'   - æœºåœºè®¢é˜…: {len(self.new_sub_list)} ä¸ª')
        if self.enable_quality_check:
            logger.info(f'\nğŸ” è´¨é‡æ§åˆ¶ç»Ÿè®¡:')
            logger.info(f'   - æ£€æŸ¥æ€»æ•°: {total} ä¸ª')
            low_quality_total = (self.quality_stats['empty_subscription'] + 
                                self.quality_stats['low_quality'] + 
                                self.quality_stats['spam_content'])
            if low_quality_total > 0:
                logger.info(f'   - ä½è´¨é‡è®¢é˜…: {low_quality_total} ä¸ª')
        if failed_count > 0:
            logger.info(f'\nâŒ å¤±æ•ˆè®¢é˜…: {failed_count} ä¸ª')
        logger.info('='*60)
    @logger.catch
    def url_check_valid(self, target, url, bar):
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨å•æ¬¡è¯·æ±‚ï¼Œä¸è¿›è¡Œé‡è¯•
        # è¿™æ ·å¯ä»¥ç¡®ä¿éå†æ‰€æœ‰åç«¯ï¼Œè€Œä¸æ˜¯åªé‡è¯•æŸä¸€ä¸ª
        success = False
        url_encode = quote(url, safe='')
        # éå†æ‰€æœ‰é…ç½®çš„åç«¯ API
        for api_url in self.check_url_list:
            try:
                check_url_string = self.check_node_url_str.format(api_url, target, url_encode)
                headers = {'User-Agent': self.get_random_ua()}
                # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼ŒåŠ å¿«è½®è¯¢é€Ÿåº¦
                res = requests.get(check_url_string, headers=headers, timeout=self.request_timeout, proxies=self.proxies)
                if res.status_code == 200:
                    with self.lock:
                        self.airport_list.append(url)
                    success = True
                    break # æˆåŠŸåˆ™åœæ­¢è½®è¯¢
            except requests.RequestException:
                continue # å½“å‰ API å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
            except Exception as e:
                logger.debug(f'è§£æå¤±è´¥: {api_url} - {type(e).__name__}')
                continue
        if not success:
            logger.warning(f'æ‰€æœ‰èŠ‚ç‚¹è½¬æ¢ API å‡ä¸å¯ç”¨æˆ–æ£€æµ‹å¤±è´¥: {mask_sensitive_data(url)[:30]}...')
            # å¦‚æœæ˜¯åˆ—è¡¨ä¸ºç©ºå¯¼è‡´æ²¡æœ‰å¾ªç¯ï¼Œä¹Ÿå±äºå¤±è´¥
            if not self.check_url_list:
                logger.warning('æ‰€æœ‰èŠ‚ç‚¹è½¬æ¢ API å‡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶')
        bar.update(1)
    def write_url_config(self, url_file, url_list, target):
        logger.info('æ£€æµ‹è®¢é˜…èŠ‚ç‚¹æœ‰æ•ˆæ€§')
        self.airport_list = []
        bar = tqdm(total=len(url_list), desc='èŠ‚ç‚¹æ£€æµ‹ï¼š')
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.url_check_valid, target, url, bar) for url in url_list]
            concurrent.futures.wait(futures)
        bar.close()
        logger.info('æ£€æµ‹è®¢é˜…èŠ‚ç‚¹æœ‰æ•ˆæ€§å®Œæˆ')
        # è¯»å–ç›´æ¥é‡‡é›†çš„èŠ‚ç‚¹
        direct_nodes = []
        if os.path.exists(self.collected_nodes_path):
            with open(self.collected_nodes_path, 'r', encoding='utf-8') as f:
                direct_nodes = f.read().splitlines()
        # åˆå¹¶æ‰€æœ‰æ¥æº
        final_list = self.airport_list + direct_nodes
        # è¿‡æ»¤ï¼šåªä¿ç•™èŠ‚ç‚¹URLï¼Œç§»é™¤è®¢é˜…é“¾æ¥
        nodes_only = []
        for item in final_list:
            item_str = str(item).strip()
            # ä¿ç•™åè®®èŠ‚ç‚¹ï¼Œæ’é™¤httpè®¢é˜…é“¾æ¥
            if '://' in item_str and not item_str.startswith(('http://', 'https://')):
                nodes_only.append(item_str)
        # Base64ç¼–ç èŠ‚ç‚¹åˆ—è¡¨
        nodes_text = '\n'.join(nodes_only)
        base64_content = base64.b64encode(nodes_text.encode('utf-8')).decode('utf-8')
        # å†™å…¥Base64ç¼–ç çš„è®¢é˜…æ–‡ä»¶ï¼ˆæ·»åŠ æ–‡ä»¶å¤§å°é™åˆ¶ï¼‰
        output_file = url_file.replace('sub_store', target)
        # æ–‡ä»¶å¤§å°é™åˆ¶ï¼š5MB
        max_file_size = 5 * 1024 * 1024  # 5MB
        content_size = len(base64_content.encode('utf-8'))
        if content_size > max_file_size:
            logger.warning(f'âš ï¸ {target} æ–‡ä»¶è¿‡å¤§ ({content_size/1024/1024:.2f}MB > 5MB)ï¼Œå°†è¿›è¡Œæ™ºèƒ½è£å‰ª...')
            # è®¡ç®—éœ€è¦ä¿ç•™çš„èŠ‚ç‚¹æ•°é‡
            keep_ratio = max_file_size / content_size
            keep_count = int(len(nodes_only) * keep_ratio * 0.95)  # ä¿ç•™95%ä»¥ç¡®ä¿ä¸è¶…é™
            # éšæœºé‡‡æ ·ä¿ç•™èŠ‚ç‚¹ï¼ˆæ›´å…¬å¹³ï¼‰
            import random
            nodes_only = random.sample(nodes_only, keep_count)
            nodes_text = '\n'.join(nodes_only)
            base64_content = base64.b64encode(nodes_text.encode('utf-8')).decode('utf-8')
            logger.info(f'ğŸ“Š å·²è£å‰ªè‡³ {keep_count} ä¸ªèŠ‚ç‚¹ ({len(base64_content.encode("utf-8"))/1024/1024:.2f}MB)')
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(base64_content)
        logger.info(f'âœ… å·²ç”Ÿæˆ {target} è®¢é˜…æ–‡ä»¶: {len(nodes_only)} ä¸ªèŠ‚ç‚¹ (Base64ç¼–ç , {len(base64_content.encode("utf-8"))/1024/1024:.2f}MB)')
    def write_sub_store(self, yaml_file):
        logger.info('å†™å…¥ sub_store æ–‡ä»¶--')
        dict_url = self.load_sub_yaml(yaml_file)
        abs_yaml_file = self.get_abs_path(yaml_file)
        play_list = dict_url['å¼€å¿ƒç©è€']
        play_url_list = re.findall(self.re_str, str(play_list))
        sub_list = dict_url['æœºåœºè®¢é˜…']
        sub_url_list = re.findall(self.re_str, str(sub_list))
        write_str = "-- play_list --\n\n\n" + '\n'.join(str(item) for item in play_url_list)
        write_str += "\n\n\n-- sub_list --\n\n\n" + '\n'.join(str(item) for item in sub_url_list)
        url_file = abs_yaml_file.replace('.yaml','_sub_store.txt')
        with open(url_file, 'w', encoding='utf-8') as f:
            f.write(write_str)
        self.write_url_config(url_file, play_url_list, 'loon')
        self.write_url_config(url_file, sub_url_list, 'clash')
    def write_merge_files(self, yaml_file):
        """ç”Ÿæˆåˆå¹¶åçš„æ–‡ä»¶"""
        # 1. æ±‡æ€»æ‰€æœ‰èŠ‚ç‚¹
        final_nodes = list(self.unique_nodes) # åŒ…å«ä»è®¢é˜…ä¸­è§£æçš„æ‰€æœ‰èŠ‚ç‚¹
        # 2. åˆå¹¶ç›´æ¥é‡‡é›†çš„èŠ‚ç‚¹ (è™½ç„¶ sub_check å·²ç»æŠŠè®¢é˜…é‡Œçš„èŠ‚ç‚¹åŠ è¿›å»äº†ï¼Œä½† collected_nodes_set æ¥è‡ªç½‘é¡µçˆ¬å–)
        final_nodes.extend(list(self.collected_nodes_set))
        # 3. å†æ¬¡å»é‡å¹¶æ’åº
        final_nodes = sorted(list(set(final_nodes)))
        # 4. å†™å…¥ sub_merge.txt (èŠ‚ç‚¹åˆ—è¡¨)
        content_merge = '\n'.join(final_nodes)
        path_merge = os.path.join(self.base_dir, 'sub_merge.txt')
        with open(path_merge, 'w', encoding='utf-8') as f:
            f.write(content_merge)
        # 5. å†™å…¥ _url_check.txt (åŒæ ·ä½¿ç”¨å»é‡åçš„èŠ‚ç‚¹é›†åˆï¼Œæ»¡è¶³ç”¨æˆ·éœ€æ±‚)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ yaml_file çš„è·¯å¾„æ¥ç¡®å®š _url_check.txt çš„ä½ç½®ï¼Œæˆ–è€…ç›´æ¥è¦†ç›–
        abs_path_yaml = self.get_abs_path(yaml_file)
        url_check_path = abs_path_yaml.replace('.yaml','_url_check.txt')
        with open(url_check_path, 'w', encoding='utf-8') as f:
            f.write(content_merge)
        # 6. å†™å…¥ base64 ç‰ˆæœ¬
        path_base64 = os.path.join(self.base_dir, 'sub_merge_base64.txt')
        with open(path_base64, 'w', encoding='utf-8') as f:
            f.write(base64.b64encode(content_merge.encode('utf-8')).decode('utf-8'))
        logger.info(f'åˆå¹¶å®Œæˆ: {len(final_nodes)} ä¸ªå”¯ä¸€èŠ‚ç‚¹å·²å†™å…¥ sub_merge.txt')
        # 6. æ›´æ–° sub_all.yaml (ä»ç„¶ä¿ç•™æœ‰æ•ˆçš„è®¢é˜…é“¾æ¥ä½œä¸ºå†å²è®°å½•)
        # æ³¨æ„ï¼šè¿™é‡Œçš„ new_sub_list ç­‰æ˜¯åœ¨ run() æµç¨‹ä¸­ populated çš„
        # å¦‚æœæ˜¯ merge_sub è°ƒç”¨ sub_updateï¼Œè¿™äº› list åŒ…å«äº†å½“å‰æœ‰æ•ˆçš„æ‰€æœ‰è®¢é˜…
        # æˆ‘ä»¬éœ€è¦è¯»å– yaml_file, ç„¶åæ›´æ–°å®ƒ
        pass # write_sub_store å·²ç»è´Ÿè´£å†™å…¥ yamlï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤å†™å…¥ yaml
    def verify_subscription_outputs(self):
        """éªŒè¯ç”Ÿæˆçš„è®¢é˜…æ–‡ä»¶æ ¼å¼ï¼ˆBase64 ä¸åè®®ï¼‰"""
        files_to_check = [
            os.path.join(self.base_dir, 'sub', 'sub_all_clash.txt'),
            os.path.join(self.base_dir, 'sub', 'sub_all_loon.txt')
        ]
        results = {}
        for filepath in files_to_check:
            results[filepath] = verify_subscription_file(filepath)
        self._append_summary(self._format_verify_summary(results))
        failed = [os.path.basename(path) for path, ok in results.items() if not ok]
        if failed:
            raise RuntimeError(f'è®¢é˜…æ–‡ä»¶éªŒè¯å¤±è´¥: {", ".join(failed)}')
        logger.info('è®¢é˜…æ–‡ä»¶éªŒè¯é€šè¿‡: sub_all_clash.txt, sub_all_loon.txt')
    def _format_verify_summary(self, results):
        lines = [
            "## Subscription Verify",
            "",
            "| File | Status |",
            "| --- | --- |"
        ]
        for path, ok in results.items():
            name = os.path.basename(path)
            status = "âœ… Passed" if ok else "âŒ Failed"
            lines.append(f"| `{name}` | {status} |")
        lines.append("")
        return "\n".join(lines)
    def _append_summary(self, content):
        summary_path = os.getenv('GITHUB_STEP_SUMMARY')
        if not summary_path:
            return
        try:
            with open(summary_path, 'a', encoding='utf-8') as f:
                f.write(content)
                if not content.endswith("\n"):
                    f.write("\n")
        except Exception as e:
            logger.warning(f'å†™å…¥ Actions Summary å¤±è´¥: {e}')
    def get_url_form_yaml(self, yaml_file):
        dict_url = self.load_sub_yaml(yaml_file)
        url_list = []
        for key in ['æœºåœºè®¢é˜…', 'clashè®¢é˜…', 'v2è®¢é˜…', 'å¼€å¿ƒç©è€']:
            url_list.extend(dict_url.get(key, []))
        url_list = re.findall(self.re_str, str(url_list))
        return [url for url in url_list if is_safe_url(url)]
    def get_url_form_channel(self):
        logger.info('è¯»å–configæˆåŠŸ')
        url_list = []
        if self.list_tg:
            logger.info(f'å¼€å§‹æŠ“å– {len(self.list_tg)} ä¸ª Telegram é¢‘é“...')
            for channel_url in self.list_tg:
                temp_list = self.fetch_urls_from_page(channel_url)
                if temp_list: url_list.extend(temp_list)
        if self.list_web_fuzz:
            logger.info(f'å¼€å§‹æ¨¡ç³ŠæŠ“å– {len(self.list_web_fuzz)} ä¸ªç½‘é¡µ...')
            for web_url in self.list_web_fuzz:
                temp_list = self.fetch_urls_from_page(web_url)
                if temp_list: url_list.extend(temp_list)
        if self.list_subscribe:
            logger.info(f'åŠ è½½ {len(self.list_subscribe)} ä¸ªç›´è¿è®¢é˜…æº...')
            url_list.extend(self.list_subscribe)
        self.save_collected_nodes()
        return url_list
    def run(self):
        start_time = time.time()
        try:
            # 1. Update Today's Sub
            url_list = self.get_url_form_channel()
            path_yaml = pre_check() # pre_check returns relative path
            self.sub_update(url_list, path_yaml)
            # 2. Merge Sub
            all_yaml = get_sub_all() # returns relative path
            # pre_check was called above, so path_yaml is valid
            merge_url_list = []
            merge_url_list.extend(self.get_url_form_yaml(all_yaml))
            merge_url_list.extend(self.get_url_form_yaml(path_yaml))
            self.sub_update(merge_url_list, all_yaml)
            self.write_sub_store(all_yaml)
            self.verify_subscription_outputs()
            self.write_merge_files(all_yaml)
            # 3. Notification
            runtime = time.time() - start_time
            runtime_str = f"{int(runtime // 60)}åˆ†{int(runtime % 60)}ç§’"
            try:
                from notification import send_notification, format_notification_message
                stats_data = {
                    'valid_count': len(self.new_sub_list) + len(self.new_clash_list) + len(self.new_v2_list),
                    'clash_count': len(self.new_clash_list),
                    'v2ray_count': len(self.new_v2_list),
                    'airport_count': len(self.new_sub_list),
                    'total_checked': self.quality_stats.get('total_checked', 0),
                    'low_quality_count': (self.quality_stats.get('low_quality', 0) + 
                                         self.quality_stats.get('empty_subscription', 0) + 
                                         self.quality_stats.get('spam_content', 0)),
                    'failed_count': len(self.failed_sub_list),
                    'runtime': runtime_str
                }
                message = format_notification_message(stats_data)
                send_notification(message, "SmartSub è¿è¡ŒæˆåŠŸ")
            except Exception as e:
                logger.warning(f'å‘é€é€šçŸ¥å¤±è´¥: {e}')
            logger.info('âœ… æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆ')
        except Exception as e:
            logger.error(f'âŒ è¿è¡Œå¤±è´¥: {e}')
            try:
                from notification import send_notification, format_error_notification
                error_msg = format_error_notification(str(e))
                send_notification(error_msg, "SmartSub è¿è¡Œå¤±è´¥")
            except:
                pass
            raise
if __name__ == '__main__':
    collector = SubscriptionCollector()
    collector.run()
